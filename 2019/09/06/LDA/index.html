<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="google-site-verification" content="iejSa-LmOb9d1GguAcEsQNUsQviccOieHkuG1c1E2YI">



  <meta name="msvalidate.01" content="83768A52AE58ADF203609FEF9C55FF47">












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="无监督学习,潜在狄利克雷分布,LDA,">










<meta name="description" content="潜在狄利克雷分布(latent Dirichlet allocation,LDA)，作为基于贝叶斯学习的话题模型，是潜在语义分析、概率潜在语义分析的扩展。 LDA模型是文本集合的生成概率模型。假设每个文本由话题的一个多项分布表示，每个话题由单词的一个多项分布表示，特别假设文本的话题分布的先验分布是狄利克雷分布，话题的单词分布的先验分布也是狄利克雷分布。 先验分布的导入使LDA能够更好的应对话题模型">
<meta name="keywords" content="无监督学习,潜在狄利克雷分布,LDA">
<meta property="og:type" content="article">
<meta property="og:title" content="潜在狄利克雷分布LDA">
<meta property="og:url" content="https://jozeelin.github.io/2019/09/06/LDA/index.html">
<meta property="og:site_name" content="Jozee&#39;s技术博客">
<meta property="og:description" content="潜在狄利克雷分布(latent Dirichlet allocation,LDA)，作为基于贝叶斯学习的话题模型，是潜在语义分析、概率潜在语义分析的扩展。 LDA模型是文本集合的生成概率模型。假设每个文本由话题的一个多项分布表示，每个话题由单词的一个多项分布表示，特别假设文本的话题分布的先验分布是狄利克雷分布，话题的单词分布的先验分布也是狄利克雷分布。 先验分布的导入使LDA能够更好的应对话题模型">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://jozeelin.github.io/Users/jozee/Desktop/image/20-1.png">
<meta property="og:image" content="https://jozeelin.github.io/Users/jozee/Desktop/image/20-2.jpg">
<meta property="og:image" content="https://jozeelin.github.io/Users/jozee/Desktop/image/20-3.png">
<meta property="og:image" content="https://jozeelin.github.io/Users/jozee/Desktop/image/20-4.png">
<meta property="og:updated_time" content="2019-09-06T15:50:53.344Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="潜在狄利克雷分布LDA">
<meta name="twitter:description" content="潜在狄利克雷分布(latent Dirichlet allocation,LDA)，作为基于贝叶斯学习的话题模型，是潜在语义分析、概率潜在语义分析的扩展。 LDA模型是文本集合的生成概率模型。假设每个文本由话题的一个多项分布表示，每个话题由单词的一个多项分布表示，特别假设文本的话题分布的先验分布是狄利克雷分布，话题的单词分布的先验分布也是狄利克雷分布。 先验分布的导入使LDA能够更好的应对话题模型">
<meta name="twitter:image" content="https://jozeelin.github.io/Users/jozee/Desktop/image/20-1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://jozeelin.github.io/2019/09/06/LDA/">





  <title>潜在狄利克雷分布LDA | Jozee's技术博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jozee's技术博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jozeelin.github.io/2019/09/06/LDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jozee">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jozee's技术博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">潜在狄利克雷分布LDA</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-06T23:29:18+08:00">
                2019-09-06
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-09-06T23:50:53+08:00">
                2019-09-06
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/09/06/LDA/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/09/06/LDA/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
          <span id="busuanzi_container_page_pv">
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon">
            <i class="fa fa-eye"></i>
            </span>
            阅读量: <span id="busuanzi_value_page_pv"></span>次
          </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>潜在狄利克雷分布(latent Dirichlet allocation,LDA)，作为<strong>基于贝叶斯学习</strong>的<strong>话题模型</strong>，<strong>是潜在语义分析、概率潜在语义分析的扩展</strong>。</p>
<p>LDA模型是文本集合的<strong>生成概率模型</strong>。假设每个文本由话题的一个多项分布表示，每个话题由单词的一个多项分布表示，特别假设文本的话题分布的<strong>先验分布是狄利克雷分布</strong>，话题的单词分布的先验分布也是狄利克雷分布。</p>
<p>先验分布的导入使LDA能够更好的应对话题模型学习中的过拟合现象。</p>
<p>LDA的文本集合的生成过程如下：首先，随机生成一个文本的话题分布，之后在该文本的每个位置，依据该文本的话题分布随机生成一个话题，然后在该位置依据该话题的单词分布生成一个单词，直至文本的最后一个位置，生成整个文本。重复以上过程生成所有文本。</p>
<p><strong>LDA模型是含有隐变量的概率图模型</strong>。每个话题的单词分布，每个文本的话题分布，文本的每个位置的话题是隐变量；文本的每个位置的单词是观测变量。</p>
<p>LDA模型的学习与推理无法直接求解，通常使用吉布斯抽样和变分EM算法(variational EM algorithm)，前者是蒙特卡罗法，而后者是近似算法。</p>
<h2 id="狄利克雷分布"><a href="#狄利克雷分布" class="headerlink" title="狄利克雷分布"></a>狄利克雷分布</h2><h3 id="分布定义"><a href="#分布定义" class="headerlink" title="分布定义"></a>分布定义</h3><h4 id="多项分布"><a href="#多项分布" class="headerlink" title="多项分布"></a>多项分布</h4><p>多项分布是一种多元离散随机变量的概率分布，是二项分布的扩展。</p>
<p>假设重复进行n次独立随机试验，每次试验可能出现的结果有k种，第i种结果出现的概率为<script type="math/tex">p_i</script>，第i种结果出现的次数为<script type="math/tex">n_i</script>。如果用随机变量<script type="math/tex">X=(X_1,X_2,\dots,X_k)</script>表示试验所有可能结果的次数，其中<script type="math/tex">X_i</script>表示第i种结果出现的次数，那么随机变量X服从多项分布。</p>
<p><strong>定义20.1(多项分布)</strong> 若多元离散变量<script type="math/tex">X=(X_1,X_2,\dots,X_k)</script>的概率质量函数为:</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(X_1=n1,X_2=n2,\dots,X_k=n_k) &= \frac{n!}{n_1!n_2!\dots n_k!}p^{n_1}_1p_2^{n_2}\dots p_k^{n_k}\\
&= \frac{n!}{\prod_{i=1}^kn_i!}\prod_{i=1}^k p_i^{n_I}
\end{aligned}\tag{20-1}</script><p>其中<script type="math/tex">p=(p_1,p_2,\dots,p_k),p_i\ge0 \ ,i=1,2,\dots,k \ , \sum_{i=1}^k p_i=1 \ , \sum_{i=1}^k n_i = n</script>，则称随机变量X服从参数为(n,p)的多项分布，记作<script type="math/tex">X \sim \mathrm{Mult}(n,p)</script>。</p>
<p>当试验的次数n为1时，多项分布变成类别分布(categorical distribution)。类别分布表示试验可能出现的k种结果的概率。</p>
<h4 id="狄利克雷分布-1"><a href="#狄利克雷分布-1" class="headerlink" title="狄利克雷分布"></a>狄利克雷分布</h4><p>狄利克雷分布是一种多元连续随机变量概率分布，是贝塔分布(beta distribution)的扩展。在贝叶斯学习中，狄利克雷分布常作为多项式分布的先验分布使用。</p>
<p><strong>定义20.2(狄利克雷分布)</strong> 若多元连续随机变量<script type="math/tex">\theta(\theta_1,\theta_2,\dots,\theta_k)</script>的概率密度函数为：</p>
<script type="math/tex; mode=display">
p(\theta|\alpha) = \frac{\varGamma\left(\sum_{i=1}^k \alpha_i\right)}{\prod_{i=1}^k \varGamma(\alpha_i)}\prod_{i=1}^k \theta_i^{\alpha_i-1} \tag{20-2}</script><p>其中<script type="math/tex">\sum_{i=1}^k \theta_i=1\ , \theta_i\ge0\ , \alpha=(\alpha_1,\alpha_2,\dots,\alpha_k) \ , \alpha_i>0 \ , i=1,2,\dots,k</script>，则称随机变量<script type="math/tex">\theta</script>服从参数为<script type="math/tex">\alpha</script>的狄利克雷分布，记作 <script type="math/tex">\theta \sim \mathrm{Dir}(\alpha)</script>。</p>
<p>式中<script type="math/tex">\varGamma(s)</script>是伽马函数，定义为：</p>
<script type="math/tex; mode=display">
\varGamma(s) = \int_0^{\infty} x^{s-1}e^{-x}dx \ , s>0</script><p>具有性质</p>
<script type="math/tex; mode=display">
\varGamma(s+1) = s\varGamma(s)</script><p>当s是自然数时，有</p>
<script type="math/tex; mode=display">
\varGamma(s+1) = s!</script><p>由于满足条件</p>
<script type="math/tex; mode=display">
\theta_i \ge 0 \ , \sum_{i=1}^k \theta_i = 1</script><p>所以狄利克雷分布<script type="math/tex">\theta</script>存在于(k-1)维单纯形上。</p>
<p>令</p>
<script type="math/tex; mode=display">
B(\alpha) = \frac{\prod_{i=1}^k \varGamma(\alpha_i)}{\varGamma\left(\sum_{i=1}^k \alpha_i\right)} \tag{20-3}</script><p>则狄利克雷分布的密度函数可以写成：</p>
<script type="math/tex; mode=display">
p(\theta|\alpha) = \frac{1}{B(\alpha)} \prod_{i=1}^k \theta_i^{\alpha_i-1} \tag{20-4}</script><p>其中，<script type="math/tex">B(\alpha)</script>是规范化因子，称为多元贝塔函数(或扩展的贝塔函数)。由密度函数的性质：</p>
<script type="math/tex; mode=display">
\int \frac{\varGamma\left(\sum_{i=1}^k \alpha_i\right)}{\prod_{i=1}^k \varGamma(\alpha_i)}\prod_{i=1}^k \theta_i^{\alpha_i-1} d\theta = \frac{\varGamma\left(\sum_{i=1}^k \alpha_i\right)}{\prod_{i=1}^k \varGamma(\alpha_i)} \int \prod_{i=1}^k \theta_i^{\alpha_i-1} d\theta = 1</script><p>得</p>
<script type="math/tex; mode=display">
B(\alpha) = \int \prod_{i=1}^k \theta_i^{\alpha_i-1} d\theta \tag{20-5}</script><p>所以式(20-5)是多元贝塔函数的积分表示。</p>
<h4 id="二项分布和贝塔分布"><a href="#二项分布和贝塔分布" class="headerlink" title="二项分布和贝塔分布"></a>二项分布和贝塔分布</h4><p>二项分布是多项分布的特殊情况，贝塔分布是狄利克雷分布的特殊情况。</p>
<p><strong>二项分布</strong>是指如下概率分布。X为离散随机变量，取值为m，其概率质量函数为:</p>
<script type="math/tex; mode=display">
P(X=m) = \begin{pmatrix}n\\m\end{pmatrix}p^m(1-p)^{n-m} \ , m=0,1,2,\dots,n \tag{20-6}</script><p>其中n和p<script type="math/tex">(0\le p \le 1)</script>是参数。</p>
<p><strong>贝塔分布</strong>是指如下概率分布，X为连续随机变量，取值范围为[0,1]，其概率密度函数为:</p>
<script type="math/tex; mode=display">
p(x) = \left\{\begin{aligned}&\frac{1}{B(s,t)}x^{s-1}(1-x)^{t-1},0\le x\le 1 \\ &0, 其它\end{aligned}\right . \tag{20-7}</script><p>其中，s&gt;0和t&gt;0是参数，<script type="math/tex">B(s,t) = \frac{\varGamma(s)\varGamma(t)}{\varGamma(s+t)}</script>是贝塔函数，定义为:</p>
<script type="math/tex; mode=display">
B(s,t) = \int_0^1 x^{s-1}(1-x)^{t-1}dx \tag{20-8}</script><p>当s,t是自然数时，</p>
<script type="math/tex; mode=display">
B(s,t) = \frac{(s-1)!(t-1)!}{(s+t-1)!} \tag{20-9}</script><p>当n=1时，二项分布变成伯努利分布或0-1分布。伯努利分布表示试验可能出现的2种结果的概率。</p>
<p>下图给出了几种概率分布的关系：</p>
<p><img src="/Users/jozee/Desktop/image/20-1.png" alt="20-1"></p>
<h3 id="共轭先验"><a href="#共轭先验" class="headerlink" title="共轭先验"></a>共轭先验</h3><p>狄利克雷分布的一些重要性质：</p>
<ol>
<li>狄利克雷分布属于指数分布族；</li>
<li>狄利克雷分布是多项分布的共轭先验</li>
</ol>
<p>贝叶斯学习中常使用共轭分布。如果后验分布与先验分布属于同类，则先验分布与后验分布称为共轭分布(conjugate distribution)，先验分布称为共轭先验(conjugate prior)。</p>
<p>作为先验分布的狄利克雷分布的参数又称为超参数。<strong>使用共轭分布的好处是便于从先验分布计算后验分布</strong>。</p>
<p>设<script type="math/tex">\mathcal{W} = \{w_1,w_2,\dots,w_k\}</script>是由k个元素组成的集合。随机变量X服从<script type="math/tex">\mathcal{W}</script>上的多项分布，<script type="math/tex">X \sim \mathrm{Mult}(n,\theta)</script>，其中<script type="math/tex">n=(n_1,n_2,\dots,n_k)</script>和<script type="math/tex">\theta=(\theta_1,\theta_2,\dots,\theta_k)</script>是参数。参数n为从<script type="math/tex">\mathcal{W}</script>中重复独立抽取样本的次数，<script type="math/tex">n_i</script>为样本中<script type="math/tex">w_i</script>出现的次数<script type="math/tex">(i=1,2,\dots,k)</script>；参数<script type="math/tex">\theta_i</script>为<script type="math/tex">w_i</script>出现的概率<script type="math/tex">(i=1,2,\dots,k)</script>。</p>
<p>将样本数据表示为D，目标是计算在样本数据D给定条件下参数<script type="math/tex">\theta</script>的后验概率<script type="math/tex">p(\theta|D)</script>。对于给定的样本数据D，似然函数是:</p>
<script type="math/tex; mode=display">
p(D|\theta) = \theta_1^{n_1}\theta_2^{n_2}\dots\theta_k^{n_k} = \prod_{i=1}^k \theta_i^{n_i} \tag{20-10}</script><p>假设随机变量<script type="math/tex">\theta</script>服从狄利克雷分布<script type="math/tex">p(\theta|\alpha)</script>，其中<script type="math/tex">\alpha=(\alpha_1,\alpha_2,\dots,\alpha_k)</script>为参数。则<script type="math/tex">\theta</script>的先验分布为：</p>
<script type="math/tex; mode=display">
p(\theta|\alpha) = \frac{\varGamma\left(\sum_{i=1}^k \alpha_i\right)}{\prod_{i=1}^k \varGamma(\alpha_i)}\prod_{i=1}^k \theta_i^{\alpha_i-1}  = \frac{1}{B(\alpha)} \prod_{i=1}^k \theta_i^{\alpha_i-1} =\mathrm{Dir}(\theta|\alpha) \ , \alpha_i > 0 \tag{20-11}</script><p>根据贝叶斯规则，在给定样本数据D和参数<script type="math/tex">\alpha</script>条件下，<script type="math/tex">\theta</script> 的后验概率分布是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(\theta|D,\alpha) &= \frac{p(D|\theta)p(\theta|\alpha)}{p(D|\alpha)}\\
&=\frac{\prod_{i=1}^k \theta_i^{n_i} \frac{1}{B(\alpha)} \prod_{i=1}^k \theta_i^{\alpha_i-1}}{\int \prod_{i=1}^k \theta_i^{n_i} \frac{1}{B(\alpha)} \prod_{i=1}^k \theta_i^{\alpha_i-1} d\theta}\\
&= \frac{1}{B(\alpha+n)} \prod_{i=1}^k \theta_i^{\alpha_i+n_i-1}\\
&= \mathrm{Dir}(\theta|\alpha+n) 
\end{aligned}\tag{20-12}</script><p>狄利克雷后验分布的参数等于狄利克雷先验分布参数<script type="math/tex">\alpha=(\alpha_1,\alpha_2,\dots,\alpha_k)</script>加上多项分布的观测计数<script type="math/tex">n=(n_1,n_2,\dots,n_k)</script>。好像试验之前就已经观察到计数<script type="math/tex">\alpha=(\alpha_1,\alpha_2,\dots,\alpha_k)</script>，因此也把<script type="math/tex">\alpha</script>叫做<strong>先验伪计数(prior pseudo-counts)</strong>。</p>
<h2 id="潜在狄利克雷分配模型"><a href="#潜在狄利克雷分配模型" class="headerlink" title="潜在狄利克雷分配模型"></a>潜在狄利克雷分配模型</h2><h3 id="基本想法"><a href="#基本想法" class="headerlink" title="基本想法"></a>基本想法</h3><p>潜在狄利克雷分配是文本集合的生成概率模型。</p>
<p>LDA模型表示文本集合的自动生成过程：</p>
<ol>
<li>首先，基于单词分布的先验分布(狄利克雷分布)生成多个单词分布，即决定多个主题内容；</li>
<li>之后，基于话题分布的先验分布(狄利克雷分布)生成多个话题分布，即决定多个文本内容；</li>
<li>然后，基于每一话题分布生成话题序列，针对每个话题，基于话题的单词分布生成单词，整体构成一个单词序列，即生成文本。重复这个过程生成所有文本。</li>
</ol>
<p>下图显示LDA的文本生成过程：</p>
<p><img src="/Users/jozee/Desktop/image/20-2.jpg" alt="20-2"></p>
<p>LDA模型是概率图模型，其特点是以狄利克雷分布为多项分布的先验分布，学习就是给定文本集合，通过后验概率分布的估计，推断模型的所有参数。利用LDA进行话题分析，就是对给定文本集合，学习到每个文本的话题分布，以及每个话题的单词分布。</p>
<p>LDA与PLSA的区别：</p>
<p><strong>不同点：</strong></p>
<ol>
<li>LDA使用狄利克雷分布作为先验分布，而PLSA不使用先验分布(或者说假设先验分布是均匀分布)，两者对文本生成过程有不同假设。</li>
<li>学习过程LDA基于贝叶斯学习，而PLSA基于极大似然估计</li>
</ol>
<p><strong>相同点：</strong>两者都假设话题是单词的多项分布，文本是话题的多项分布。</p>
<p><strong>LDA的优点是，使用先验概率分布，可以防止学习过程中产生的过拟合</strong>。</p>
<h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><h4 id="模型要素"><a href="#模型要素" class="headerlink" title="模型要素"></a>模型要素</h4><p>潜在狄利克雷分配使用三种集合:</p>
<ol>
<li>单词集合<script type="math/tex">\mathcal{W} = \{w_1,\dots,w_v,\dots,w_V\}</script>，其中<script type="math/tex">w_v</script>是第v个单词，<script type="math/tex">v=1,2,\dots,V</script>，V是单词的个数。</li>
<li>文本集合<script type="math/tex">D = \{\boldsymbol{w}_1,\dots,\boldsymbol{w}_m,\dots,\boldsymbol{w}_M\}</script>，其中<script type="math/tex">\boldsymbol{w}_m</script>是第m个文本,m=1,2,…,M，M是文本的个数。文本<script type="math/tex">\boldsymbol{w}_m</script>是一个单词序列<script type="math/tex">\boldsymbol{w}_m = (w_{m1},\dots,w_{mn},\dots,w_{mN_m})</script>，其中<script type="math/tex">w_{mn}</script>是文本<script type="math/tex">\boldsymbol{w}_m</script>的第n个单词，<script type="math/tex">n=1,2,\dots,N_m</script>，<script type="math/tex">N_m</script>是文本<script type="math/tex">\boldsymbol{w}_m</script>中单词的个数。</li>
<li>话题集合<script type="math/tex">Z = \{z_1,\dots,z_k,\dots,z_K\}</script>，其中<script type="math/tex">z_k</script>是第k个话题，k=1,2,…,K，K是话题的个数。</li>
</ol>
<p><strong>每个话题<script type="math/tex">z_k</script>由一个单词的条件概率分布<script type="math/tex">p(w|z_k)</script>决定，<script type="math/tex">w\in W</script>。</strong>分布<script type="math/tex">p(w|z_k)</script>服从多项分布(严格意义上类别分布)，其参数为<script type="math/tex">\varphi_k</script>。</p>
<p>参数<script type="math/tex">\varphi_k</script>服从狄利克雷分布(先验分布)，其超参数为<script type="math/tex">\beta</script>。参数<script type="math/tex">\varphi_k</script>是一个V维向量<script type="math/tex">\varphi_k=(\varphi_{k1},\varphi_{k2},\dots,\varphi_{kV})</script>，其中<script type="math/tex">\varphi_{kv}</script>表示话题<script type="math/tex">z_k</script>生成单词<script type="math/tex">w_v</script>的概率。</p>
<p>所有话题的参数向量构成一个KxV矩阵<script type="math/tex">\boldsymbol{\varphi} = \{\varphi_k\}_{k=1}^K</script>。超参数<script type="math/tex">\beta</script>也是一个V维向量<script type="math/tex">\beta = (\beta_1,\beta_2,\dots,\beta_V)</script>。</p>
<p><strong>每个文本<script type="math/tex">\boldsymbol{w}_m</script>由一个话题的条件概率分布<script type="math/tex">p(z|\boldsymbol{w}_m)</script>决定的，<script type="math/tex">z \in Z</script>。</strong>分布<script type="math/tex">p(z|\boldsymbol{w}_m)</script>服从多项分布(严格意义上类别分布)，其参数为<script type="math/tex">\theta_m</script>。</p>
<p>参数<script type="math/tex">\theta_m</script>服从狄利克雷分布(先验分布)，其超参数为<script type="math/tex">\alpha</script>。参数<script type="math/tex">\theta_m</script>是一个K维向量<script type="math/tex">\theta_m = (\theta_{m1},\theta_{m2},\dots,\theta_{mK})</script>，其中<script type="math/tex">\theta_{mk}</script>表示文本<script type="math/tex">\boldsymbol{w}_m</script>生成话题<script type="math/tex">z_k</script>的概率。</p>
<p>所有文本的参数向量构成一个MxK矩阵<script type="math/tex">\boldsymbol{\theta} = \{\theta_m\}_{m=1}^M</script>。超参数<script type="math/tex">\alpha</script>也是一个K维向量<script type="math/tex">\alpha=(\alpha_1,\alpha_2,\dots,\alpha_K)</script>。</p>
<p><strong>每个文本<script type="math/tex">\boldsymbol{w}_m</script>中的每个单词<script type="math/tex">w_{mn}</script>由该文本的话题分布<script type="math/tex">p(z|\boldsymbol{w}_m)</script>以及所有话题的单词分布<script type="math/tex">p(w|z_k)</script>决定</strong>。</p>
<h4 id="生成过程"><a href="#生成过程" class="headerlink" title="生成过程"></a>生成过程</h4><p>LDA文本集合的生成过程如下：</p>
<p>给定单词集合W，文本集合D，话题集合Z，狄利克雷分布的超参数<script type="math/tex">\alpha</script>和<script type="math/tex">\beta</script>。</p>
<ol>
<li><p>生成话题的单词分布</p>
<p><strong>随机生成K个话题的单词分布</strong>。具体过程如下：</p>
<p>按照狄利克雷分布<script type="math/tex">\mathrm{Dir}(\beta)</script>随机生成一个参数向量<script type="math/tex">\varphi_k</script>，<script type="math/tex">\varphi \sim \mathrm{Dir}(\beta)</script>，作为话题<script type="math/tex">z_k</script>的单词分布<script type="math/tex">p(w|z_k),w \in W</script> ，k=1,2,…,K</p>
</li>
<li><p>生成文本的话题分布</p>
<p><strong>随机生成M个文本的话题分布</strong>。具体过程如下：</p>
<p>按照狄利克雷分布Dir(<script type="math/tex">\alpha</script>)随机生成一个参数向量<script type="math/tex">\theta_m</script>，<script type="math/tex">\theta_m \sim \mathrm{Dir}(\alpha)</script>，作为文本<script type="math/tex">\boldsymbol{w}_m</script>的话题分布<script type="math/tex">p(z|\boldsymbol{w}_m)</script>，m=1,2,…,M。</p>
</li>
<li><p>生成文本的单词序列</p>
<p><strong>随机生成M个文本的<script type="math/tex">N_m</script>个单词</strong>。文本<script type="math/tex">\boldsymbol{w}_m</script>(m=1,2,…,M)的单词<script type="math/tex">w_{mn}(n=1,2,\dots,N_m)</script>的生成过程如下：</p>
<ol>
<li>首先，按照多项分布Mult(<script type="math/tex">\theta_m</script>)随机生成一个话题<script type="math/tex">z_{mn}\ , z_{mn} \sim \mathrm{Mult}(\theta_m)</script>。</li>
<li>然后，按照多项分布Mult(<script type="math/tex">\varphi_{z_{mn}}</script>)随机生成一个单词<script type="math/tex">w_{mn}\ , w_{mn}\sim \mathrm{Mult}(\varphi_{z_{mn}})</script>。</li>
</ol>
<p>文本<script type="math/tex">\boldsymbol{w}_m</script>本身是单词序列<script type="math/tex">\boldsymbol{w}_m = (w_{m1},\dots,w_{mn},\dots,w_{mN_m})</script>，对应着隐式的话题序列:<script type="math/tex">\boldsymbol{z}_m = (z_{m1},z_{m2},\dots,z_{mN_m})</script>。</p>
</li>
</ol>
<p><strong>算法20.1(LDA的文本生成算法)</strong> </p>
<ol>
<li><p>对于话题<script type="math/tex">z_k(k=1,2,\dots,K)</script>：</p>
<p>生成多项分布参数<script type="math/tex">\varphi_k \sim \mathrm{Dir}(\beta)</script>，作为话题的单词分布<script type="math/tex">p(w|z_k)</script></p>
</li>
<li><p>对于文本<script type="math/tex">\boldsymbol{w}_m(m=1,2,\dots,M)</script>：</p>
<p>生成多项分布参数<script type="math/tex">\theta_m \sim \mathrm{Dir}(\alpha)</script>，作为文本的话题分布<script type="math/tex">p(z|\boldsymbol{w}_m)</script>。</p>
</li>
<li><p>对于文本<script type="math/tex">\boldsymbol{w}_m</script>的单词<script type="math/tex">w_{mn}(m=1,2,\dots,M,n=1,2,\dots,N_m)</script></p>
<ol>
<li>生成话题<script type="math/tex">z_{mn} \sim \mathrm{Mult}(\theta_m)</script>，作为单词对应的话题。</li>
<li>生成单词<script type="math/tex">w_{mn}\sim \mathrm{Mult}(\varphi_{z_{mn}})</script></li>
</ol>
</li>
</ol>
<p>LDA的文本生成过程中，<strong>假定话题个数K给定(通常通过试验给定)，狄利克雷分布的超参数<script type="math/tex">\alpha</script>和<script type="math/tex">\beta</script>通常也是事先给定的(在没有其它先验知识情况下,可以假设向量<script type="math/tex">\alpha</script>和<script type="math/tex">\beta</script>的所有分量均为1)</strong>。这时的文本的话题分布<script type="math/tex">\theta_m</script>是对称的，话题的单词分布<script type="math/tex">\varphi_k</script>也是对称的。</p>
<h3 id="概率图模型"><a href="#概率图模型" class="headerlink" title="概率图模型"></a>概率图模型</h3><p>如下图所示，结点表示随机变量，双边圆是观测变量，单边圆是隐变量；有向边表示概率依存关系；矩形表示重复，矩形内的数字表示重复次数。</p>
<p><img src="/Users/jozee/Desktop/image/20-3.png" alt="20-3"></p>
<p>上图的LDA板块表示，结点<script type="math/tex">\alpha</script>和<script type="math/tex">\beta</script>是模型的超参数，结点<script type="math/tex">\varphi_k</script>表示话题<script type="math/tex">z_k</script>的单词分布的参数，结点<script type="math/tex">\theta_m</script>表示文本<script type="math/tex">\boldsymbol{w}_m</script>的话题分布的参数，结点<script type="math/tex">z_{mn}</script>表示话题，结点<script type="math/tex">w_{mn}</script>表示单词。</p>
<p>结点<script type="math/tex">\beta</script>指向结点<script type="math/tex">\varphi_k</script>，重复K次，表示根据超参数<script type="math/tex">\beta</script>生成K个话题的单词分布的参数<script type="math/tex">\varphi_k</script>;</p>
<p>结点<script type="math/tex">\alpha</script>指向结点<script type="math/tex">\theta_m</script>，重复M次，表示根据超参数<script type="math/tex">\alpha</script>生成M个文本的话题分布的参数<script type="math/tex">\theta_m</script>；</p>
<p>结点<script type="math/tex">\theta_m</script>指向结点<script type="math/tex">z_{mn}</script>，重复<script type="math/tex">N_m</script>次，表示根据文本的话题分布<script type="math/tex">\theta_m</script>生成<script type="math/tex">N_m</script>个话题<script type="math/tex">z_{mn}</script>；</p>
<p>结点<script type="math/tex">z_{mn}</script>指向结点<script type="math/tex">w_{mn}</script>，同时K个结点<script type="math/tex">\varphi_k</script>也指向结点<script type="math/tex">w_{mn}</script>，表示根据话题<script type="math/tex">z_{mn}</script>以及K个话题的单词分布<script type="math/tex">\varphi_k</script>生成单词<script type="math/tex">w_{mn}</script>。</p>
<p>把板块图展开得到以下的有向图表示形式：</p>
<p><img src="/Users/jozee/Desktop/image/20-4.png" alt="20-4"></p>
<h3 id="随机变量序列的可交换性"><a href="#随机变量序列的可交换性" class="headerlink" title="随机变量序列的可交换性"></a>随机变量序列的可交换性</h3><p>一个有限的随机变量序列是可交换的，是指随机变量的联合概率分布对随机变量的排列不变：</p>
<script type="math/tex; mode=display">
P(x_1,x_2,\dots,x_N) = P(x_{\pi(1)},x_{\pi(2)},\dots,x_{\pi(N)}) \tag{20-13}</script><p>这里<script type="math/tex">\pi(1),\pi(2),\dots,\pi(N)</script>表示自然数1,2,…,N的任意一个排列。一个无限的随机变量序列是无限可交换的，是指它的任意一个有限子序列都是可交换的。</p>
<p>如果一个随机变量序列<script type="math/tex">X_1,X_2,\dots,X_N,\dots</script>是<strong>独立同分布的</strong>，那么它们是<strong>无限可交换</strong>的。反之不然。</p>
<p>根据De Finetti定理，任意一个无限可交换的随机变量序列对一个随机参数是条件独立同分布的。即任意一个无限可交换的随机变量序列<script type="math/tex">X_1,X_2,\dots,X_i,\dots</script>的i基于一个随机参数Y的条件概率，等于基于这个随机参数Y的各个随机变量<script type="math/tex">X_1,X_2,\dots,X_i,\dots</script>的条件概率的乘积：</p>
<script type="math/tex; mode=display">
P(X_1,X_2,\dots,X_i,\dots|Y) = P(X_1|Y)P(X_2|Y)\dots P(X_i|Y)\dots \tag{20-14}</script><p><strong>LDA的假设前提为：文本中的话题对一个随机参数是条件独立同分布的</strong>。因此，在参数给定的情况下，文本中的话题的顺序可以忽略。</p>
<h3 id="概率公式"><a href="#概率公式" class="headerlink" title="概率公式"></a>概率公式</h3><p>LDA模型整体是由观测变量和隐变量组成的联合概率分布，可以表示为：</p>
<script type="math/tex; mode=display">
p(\boldsymbol{w},\boldsymbol{z},\theta,\varphi|\alpha,\beta) = \prod_{k=1}^K p(\varphi_k|\beta)\prod_{m=1}^Mp(\theta_m|\alpha)\prod_{n=1}^{N_m}p(z_{mn}|\theta_m)p(w_{mn}|z_{mn},\varphi) \tag{20-15}</script><p>其中观测变量<script type="math/tex">\boldsymbol{w}</script>表示所有文本中的单词序列，隐变量<script type="math/tex">\boldsymbol{z}</script>表示所有文本中的话题序列，隐变量<script type="math/tex">\theta</script>表示所有文本的话题分布的参数，隐变量<script type="math/tex">\varphi</script>表示所有话题的单词分布的参数，<script type="math/tex">\alpha</script>和<script type="math/tex">\beta</script>是超参数。</p>
<ul>
<li><script type="math/tex">p(\varphi_k|\beta)</script>表示超参数<script type="math/tex">\beta</script>给定的情况下，第k个话题的单词分布的参数<script type="math/tex">\varphi_k</script>的生成概率；</li>
<li><script type="math/tex">p(\theta_m|\alpha)</script>表示超参数<script type="math/tex">\alpha</script>给定的情况下，第m个文本的话题分布的参数<script type="math/tex">\theta_m</script>的生成概率；</li>
<li><script type="math/tex">p(z_{mn}|\theta_m)</script>表示第m个文本的话题分布<script type="math/tex">\theta_m</script>给定的情况下，文本的第n个位置的话题<script type="math/tex">z_{mn}</script>的生成概率；</li>
<li><script type="math/tex">p(w_{mn}|z_{mn},\varphi)</script>，表示在第m个文本的第n个位置的话题<script type="math/tex">z_{mn}</script>及所有话题的单词分布的参数<script type="math/tex">\varphi</script>给定的情况下，第m个文本的第n个位置的单词<script type="math/tex">w_{mn}</script>的生成概率。</li>
</ul>
<p>第m个文本的联合概率分布可以表示为：</p>
<script type="math/tex; mode=display">
p(\boldsymbol{w}_m,\boldsymbol{z}_m,\theta_m,\varphi|\alpha,\beta) = \prod_{k=1}^K p(\varphi_k|\beta)p(\theta_m|\alpha)\prod_{n=1}^{N_m}p(z_{mn}|\theta_m)p(w_{mn}|z_{mn},\varphi) \tag{20-16}</script><p>其中，<script type="math/tex">\boldsymbol{w}_m</script>表示该文本中的单词序列，<script type="math/tex">\boldsymbol{z}_m</script>表示该文本的话题序列，<script type="math/tex">\theta_m</script>表示该文本的话题分布参数。</p>
<p>LDA模型的联合分布含有隐变量，对隐变量进行积分得到边缘分布。</p>
<p>参数<script type="math/tex">\theta_m,\varphi</script>给定的情况下，第m个文本的生成概率是：</p>
<script type="math/tex; mode=display">
p(\boldsymbol{w}_m|\theta_m,\varphi)=\prod_{n=1}^{N_m}\left[\sum_{k=1}^Kp(z_{mn}=k|\theta_m)p(w_{mn}|\varphi_k)\right] \tag{20-17}</script><p>超参数<script type="math/tex">\alpha , \beta</script>给定的条件下，第m个文本的生成概率是：</p>
<script type="math/tex; mode=display">
p(\boldsymbol{w}_m|\alpha,\beta) = \prod_{k=1}^K \int p(\varphi_k|\beta)\left\{\int p(\theta_m|\alpha)\prod_{n=1}^{N_m}\left[\sum_{l=1}^Kp(z_{mn}=l|\theta_m)p(w_{mn}|\varphi_l)\right]d\theta_m \right\}d\varphi_k \tag{20-18}</script><p>超参数<script type="math/tex">\alpha , \beta</script>给定的条件下，所有文本的生成概率是</p>
<script type="math/tex; mode=display">
p(\boldsymbol{w}|\alpha,\beta) = \prod_{k=1}^K \int p(\varphi_k|\beta)\left\{\prod_{m=1}^M\int p(\theta_m|\alpha)\prod_{n=1}^{N_m}\left[\sum_{l=1}^Kp(z_{mn}=l|\theta_m)p(w_{mn}|\varphi_l)\right]d\theta_m \right\}d\varphi_k \tag{20-19}</script><h2 id="LDA的吉布斯抽样算法"><a href="#LDA的吉布斯抽样算法" class="headerlink" title="LDA的吉布斯抽样算法"></a>LDA的吉布斯抽样算法</h2><p>潜在狄利克雷分布的学习(参数估计)是一个复杂的最优化问题，很难精确求解，只能近似求解。</p>
<p>常用的近似求解方法有吉布斯抽样和变分推理。</p>
<p>吉布斯抽样优点是实现简单，缺点是收敛速度慢。</p>
<h3 id="基本想法-1"><a href="#基本想法-1" class="headerlink" title="基本想法"></a>基本想法</h3><p>给定文本(单词序列)的集合<script type="math/tex">D = \{\boldsymbol{w}_1,\dots,\boldsymbol{w}_m,\dots,\boldsymbol{w}_M\}</script>，其中<script type="math/tex">\boldsymbol{w}_m</script>是第m个文本,m=1,2,…,M，M是文本的个数。文本<script type="math/tex">\boldsymbol{w}_m</script>是一个单词序列<script type="math/tex">\boldsymbol{w}_m = (w_{m1},\dots,w_{mn},\dots,w_{mN_m})</script>，其中<script type="math/tex">w_{mn}</script>是文本<script type="math/tex">\boldsymbol{w}_m</script>的第n个单词，<script type="math/tex">n=1,2,\dots,N_m</script>，<script type="math/tex">N_m</script>是文本<script type="math/tex">\boldsymbol{w}_m</script>中单词的个数。</p>
<p>超参数<script type="math/tex">\alpha,\beta</script>已知。目标是要推断：</p>
<ol>
<li>话题序列的集合<script type="math/tex">\boldsymbol{z}=\{\boldsymbol{z}_1,\dots,\boldsymbol{z}_m,\dots,\boldsymbol{z}_M\}</script>的后验概率分布，其中<script type="math/tex">\boldsymbol{z}_m</script>是第m个文本的话题序列，<script type="math/tex">\boldsymbol{z}_m = (z_{m1},z_{m2},\dots,z_{mN_m})</script>；</li>
<li>参数<script type="math/tex">\theta=\{\theta_1,\dots,\theta_m,\dots,\theta_M\}</script>，其中<script type="math/tex">\theta_m</script>是文本<script type="math/tex">\boldsymbol{w}_m</script>的话题分布的参数；</li>
<li>参数<script type="math/tex">\varphi=\{\varphi_1,\dots,\varphi_k,\dots,\varphi_K\}</script>，其中<script type="math/tex">\varphi_k</script>是话题<script type="math/tex">z_k</script>的单词分布的参数。</li>
</ol>
<p>也就是说，要对联合概率分布<script type="math/tex">p(\boldsymbol{w},\boldsymbol{z},\theta,\varphi|\alpha,\beta)</script>进行估计，其中<script type="math/tex">\boldsymbol{w}</script>是观测变量，而<script type="math/tex">\boldsymbol{z},\theta,\varphi</script>是隐变量。</p>
<blockquote>
<p>使用吉布斯抽样对多元随机变量x的联合分布进行估计：</p>
<p>为了估计多元随机变量x的联合分布p(x)，吉布斯抽样法选择x的一个分量，固定其它分量，按照其条件概率分布进行随机抽样，依次循环对每一分量执行这个操作，得到联合分布p(x)的一个随机样本，重复这个过程，在燃烧期之后，得到联合概率分p(x)的样本集合。</p>
</blockquote>
<p>LDA模型的学习通常采用收缩的吉布斯抽样(collapsed Gibbs sampling)方法。基本想法是，</p>
<ol>
<li>通过对隐变量<script type="math/tex">\theta</script>和<script type="math/tex">\varphi</script>积分，得到边缘概率分布<script type="math/tex">p(\boldsymbol{w},\boldsymbol{z}|\alpha,\beta)</script>，其中<script type="math/tex">\boldsymbol{w}</script>是可观测变量，变量<script type="math/tex">\boldsymbol{z}</script>是e不可观测的；</li>
<li>对后验概率分布<script type="math/tex">p(\boldsymbol{z}|\boldsymbol{w},\alpha,\beta)</script>进行吉布斯抽样，得到分布<script type="math/tex">p(\boldsymbol{z}|\boldsymbol{w},\alpha,\beta)</script>的样本集合；</li>
<li>利用这个样本集合对参数<script type="math/tex">\theta</script>和<script type="math/tex">\varphi</script>进行估计，最终得到LDA模型<script type="math/tex">p(\boldsymbol{w},\boldsymbol{z},\theta,\varphi|\alpha,\beta)</script>的所有参数估计。</li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/无监督学习/" rel="tag"># 无监督学习</a>
          
            <a href="/tags/潜在狄利克雷分布/" rel="tag"># 潜在狄利克雷分布</a>
          
            <a href="/tags/LDA/" rel="tag"># LDA</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/09/06/mcmc/" rel="next" title="马尔可夫链蒙特卡罗法">
                <i class="fa fa-chevron-left"></i> 马尔可夫链蒙特卡罗法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">jozee</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">39</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">83</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#狄利克雷分布"><span class="nav-number">1.</span> <span class="nav-text">狄利克雷分布</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#分布定义"><span class="nav-number">1.1.</span> <span class="nav-text">分布定义</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#多项分布"><span class="nav-number">1.1.1.</span> <span class="nav-text">多项分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#狄利克雷分布-1"><span class="nav-number">1.1.2.</span> <span class="nav-text">狄利克雷分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#二项分布和贝塔分布"><span class="nav-number">1.1.3.</span> <span class="nav-text">二项分布和贝塔分布</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#共轭先验"><span class="nav-number">1.2.</span> <span class="nav-text">共轭先验</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#潜在狄利克雷分配模型"><span class="nav-number">2.</span> <span class="nav-text">潜在狄利克雷分配模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本想法"><span class="nav-number">2.1.</span> <span class="nav-text">基本想法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型定义"><span class="nav-number">2.2.</span> <span class="nav-text">模型定义</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#模型要素"><span class="nav-number">2.2.1.</span> <span class="nav-text">模型要素</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#生成过程"><span class="nav-number">2.2.2.</span> <span class="nav-text">生成过程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#概率图模型"><span class="nav-number">2.3.</span> <span class="nav-text">概率图模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机变量序列的可交换性"><span class="nav-number">2.4.</span> <span class="nav-text">随机变量序列的可交换性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#概率公式"><span class="nav-number">2.5.</span> <span class="nav-text">概率公式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LDA的吉布斯抽样算法"><span class="nav-number">3.</span> <span class="nav-text">LDA的吉布斯抽样算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本想法-1"><span class="nav-number">3.1.</span> <span class="nav-text">基本想法</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">jozee</span>

  
</div>
<div class="busuanzi_count">
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
    </script>
    <span> 本站访客数:<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
    <span>本站总访问量:<span id="busuanzi_value_site_pv"></span>次</span>
</div>









        






        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://jozeelin.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://jozeelin.github.io/2019/09/06/LDA/';
          this.page.identifier = '2019/09/06/LDA/';
          this.page.title = '潜在狄利克雷分布LDA';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://jozeelin.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
