<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="学习算法,">










<meta name="description" content="EM算法是一种迭代算法。用于含有隐变量(hidden variable)的概率模型参数的极大似然估计，或极大后验概率估计。 EM算法每次迭代由两步组成:  E步，求期望 M步，求极大  因此，EM算法也称为期望极大算法。 本章首先叙述EM算法，然后讨论算法的收敛性，接着介绍EM算法中的高斯混合模型的学习，最后叙述EM算法的推广——GEM算法。 12.1 EM算法的引入概率模型有时既含有观测变量，又">
<meta name="keywords" content="学习算法">
<meta property="og:type" content="article">
<meta property="og:title" content="EM算法及其推广">
<meta property="og:url" content="http://yoursite.com/2019/06/14/EM算法及其推广/index.html">
<meta property="og:site_name" content="Jozee&#39;s技术博客">
<meta property="og:description" content="EM算法是一种迭代算法。用于含有隐变量(hidden variable)的概率模型参数的极大似然估计，或极大后验概率估计。 EM算法每次迭代由两步组成:  E步，求期望 M步，求极大  因此，EM算法也称为期望极大算法。 本章首先叙述EM算法，然后讨论算法的收敛性，接着介绍EM算法中的高斯混合模型的学习，最后叙述EM算法的推广——GEM算法。 12.1 EM算法的引入概率模型有时既含有观测变量，又">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/06/14/EM算法及其推广/image/ch12-1.png">
<meta property="og:updated_time" content="2019-07-04T09:18:13.237Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="EM算法及其推广">
<meta name="twitter:description" content="EM算法是一种迭代算法。用于含有隐变量(hidden variable)的概率模型参数的极大似然估计，或极大后验概率估计。 EM算法每次迭代由两步组成:  E步，求期望 M步，求极大  因此，EM算法也称为期望极大算法。 本章首先叙述EM算法，然后讨论算法的收敛性，接着介绍EM算法中的高斯混合模型的学习，最后叙述EM算法的推广——GEM算法。 12.1 EM算法的引入概率模型有时既含有观测变量，又">
<meta name="twitter:image" content="http://yoursite.com/2019/06/14/EM算法及其推广/image/ch12-1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/06/14/EM算法及其推广/">





  <title>EM算法及其推广 | Jozee's技术博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jozee's技术博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/14/EM算法及其推广/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jozee">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jozee's技术博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">EM算法及其推广</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-14T17:32:10+08:00">
                2019-06-14
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/14/EM算法及其推广/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/14/EM算法及其推广/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          <span id="busuanzi_container_page_pv">
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon">
            <i class="fa fa-eye"></i>
            </span>
            阅读量: <span id="busuanzi_value_page_pv"></span>次
          </span>

          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>EM算法是一种<strong>迭代算法</strong>。<strong>用于含有隐变量(hidden variable)的概率模型参数的极大似然估计，或极大后验概率估计</strong>。</p>
<p>EM算法每次迭代由两步组成:</p>
<ul>
<li>E步，求期望</li>
<li>M步，求极大</li>
</ul>
<p>因此，EM算法也称为<strong>期望极大算法</strong>。</p>
<p>本章首先叙述EM算法，然后讨论算法的收敛性，接着介绍EM算法中的高斯混合模型的学习，最后叙述EM算法的推广——GEM算法。</p>
<h2 id="12-1-EM算法的引入"><a href="#12-1-EM算法的引入" class="headerlink" title="12.1 EM算法的引入"></a>12.1 EM算法的引入</h2><p>概率模型有时既含有<strong>观测变量</strong>，又含有<strong>隐变量或潜在变量</strong>。</p>
<p>如果概率模型的变量<strong>都是观测变量</strong>，那么给定数据，可以<strong>直接用极大似然估计法或贝叶斯估计法估计模型参数</strong>。</p>
<p>如果概率模型<strong>含有隐变量</strong>，则<strong>不能简单地使用以上估计方法</strong>。</p>
<p><strong>EM算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率估计法</strong>。</p>
<p>在这里，我们只讨论极大似然估计法。</p>
<h3 id="12-1-1-EM算法"><a href="#12-1-1-EM算法" class="headerlink" title="12.1.1 EM算法"></a>12.1.1 EM算法</h3><p>一般地，用Y表示观测随机变量的数据，Z表示隐随机变量的数据。Y和Z连在一起称为完全数据，观测数据Y又称为不完全数据。</p>
<p>假设给定观测数据Y，其概率分布是<script type="math/tex">P(Y|\theta)</script>，其中<script type="math/tex">\theta</script>是需要估计的模型参数，那么不完全数据Y的似然函数是<script type="math/tex">P(Y|\theta)</script>，对数似然函数<script type="math/tex">L(\theta)=\log P(Y|\theta)</script>；假设Y和Z的联合概率分布是<script type="math/tex">P(Y,Z|\theta)</script>，那么完全数据的对数似然函数是<script type="math/tex">\log P(Y,Z|\theta)</script>。</p>
<p>EM算法通过迭代求<script type="math/tex">L(\theta)=\log P(Y|\theta)</script>的极大似然估计。每次迭代包含两步：E步，求期望；M步，求极大化。</p>
<p><strong>算法12.1(EM算法)</strong> </p>
<p>输入：观测变量数据Y，隐变量数据Z，联合分布<script type="math/tex">P(Y,Z|\theta)</script>，条件分布<script type="math/tex">P(Z|Y,\theta)</script>；</p>
<p>输出：模型参数<script type="math/tex">\theta</script>。</p>
<p>(1)选择参数的初值<script type="math/tex">\theta^{(0)}</script>，开始迭代；</p>
<p>(2)E步：记<script type="math/tex">\theta^{(i)}</script>为第i次迭代参数<script type="math/tex">\theta</script>的估计值，在第i+1次迭代的E步，计算：</p>
<script type="math/tex; mode=display">
\begin{aligned} Q(\theta,\theta^{(i)}) &= E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}] \\ &= \sum_Z \log P(Y,Z|\theta)P(Z|Y,\theta^{(i)}) \ (12.1)\end{aligned}</script><p>这里，<script type="math/tex">P(Z|Y,\theta^{(i)}</script>是在给定观测数据Y和当前的参数估计<script type="math/tex">\theta^{(i)}</script>下隐变量数据Z的条件概率分布；</p>
<p>(3)M步：求使<script type="math/tex">Q(\theta,\theta^{(i)})</script>极大化的<script type="math/tex">\theta</script>，确定第i+1次迭代的参数的估计值<script type="math/tex">\theta^{(i+1)}</script></p>
<script type="math/tex; mode=display">
\theta^{(i+1)} = \arg \max_{\theta} Q(\theta,\theta^{(i)}) \ (12.2)</script><p>(4)重复第(2)步和第(3)步，直到收敛。</p>
<p>式(12.1)的函数<script type="math/tex">Q(\theta,\theta^{(i)})</script>是EM算法的核心，称为Q函数。</p>
<p><strong>定义12.1(Q函数)</strong> 完全数据的对数似然函数<script type="math/tex">\log P(Y,Z|\theta)</script>关于在给定观测数据Y和当前参数<script type="math/tex">\theta^{(i)}</script>下对未观测数据Z的条件概率分布<script type="math/tex">P(Z|Y,\theta^{(i)})</script>的期望称为Q函数，即</p>
<script type="math/tex; mode=display">
Q(\theta,\theta^{(i)}) = E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}] \ (12.3)</script><p><strong>下面关于EM算法作几点说明：</strong></p>
<p>步骤(1) 参数的初值可以任意选择，但需注意<strong>EM算法对初值是敏感的</strong>。</p>
<p>步骤(2) E步求<script type="math/tex">Q(\theta,\theta^{(i)})</script>。Q函数式中Z是未观测数据，Y是观测数据。(<strong>注意：</strong><script type="math/tex">Q(\theta,\theta^{(i)})</script>的第1个变量表示要极大化的参数，第2个变量表示参数的当前估计值)。每次迭代实际在求Q函数及其极大。</p>
<p>步骤(3) M步求<script type="math/tex">Q(\theta,\theta^{(i)})</script>的极大化，得到<script type="math/tex">\theta^{(i+1)}</script>，完成一次迭代<script type="math/tex">\theta^{(i)} \rightarrow \theta^{(i+1)}</script>，后面将证明每次迭代使似然函数增大或达到局部极值。</p>
<p>步骤(4) 给出停止迭代的条件，一般是对较小的正数<script type="math/tex">\varepsilon_1</script>，<script type="math/tex">\varepsilon_2</script>，若满足</p>
<script type="math/tex; mode=display">
\|\theta^{(i+1)}-\theta^{(i)}\| < \varepsilon_1</script><p>或者</p>
<script type="math/tex; mode=display">
\|Q(\theta^{(i+1)},\theta^{(i)})-Q(\theta^{(i)},\theta^{(i)})\|<\varepsilon_2</script><p>则停止迭代。</p>
<h3 id="12-1-2-EM算法的导出"><a href="#12-1-2-EM算法的导出" class="headerlink" title="12.1.2 EM算法的导出"></a>12.1.2 EM算法的导出</h3><p><strong>为什么EM算法能够近似实现对观测数据的极大似然估计呢</strong>？</p>
<p>下面通过近似求解观测数据的对数似然函数的极大化问题来导出EM算法，由此可以清楚地看出EM算法的作用。</p>
<p>对于一个含有隐变量的概率模型，<strong>目标是极大化观测数据(不完全数据)Y关于参数<script type="math/tex">\theta</script>的对数似然函数</strong>，即极大化</p>
<script type="math/tex; mode=display">
\begin{aligned} L(\theta) &= \log P(Y|\theta) = \log \sum_Z P(Y,Z|\theta)\\&= \log \left(\sum_Z P(Y|Z,\theta)P(Z|\theta) \right) \end{aligned}  \ (12.4)</script><p>式(12.4)中有未观测数据并有包含和(或积分)的对数。</p>
<p>事实上，EM算法是通过迭代逐步近似极大化<script type="math/tex">L(\theta)</script>的。假设在第i次迭代后<script type="math/tex">\theta</script>的估计值是<script type="math/tex">\theta^{(i)}</script>。我们希望新估计值<script type="math/tex">\theta</script>能使<script type="math/tex">L(\theta)</script>增加，即<script type="math/tex">L(\theta)>L(\theta^{(i)})</script>，并逐步达到极大值。为此，考虑两者的差:</p>
<script type="math/tex; mode=display">
L(\theta)-L(\theta^{(i)}) = \log \left(\sum_Z P(Y|Z,\theta)P(Z|\theta)\right)-\log P(Y|\theta^{(i)})</script><p>利用Jensen不等式得到其下界:</p>
<blockquote>
<p><strong>Jensen不等式：</strong><script type="math/tex">\log \sum_j \lambda_j y_j \ge \sum_j \lambda_j \log y_j</script>，其中<script type="math/tex">\lambda_j \ge 0</script>，<script type="math/tex">\sum_j \lambda_j = 1</script>。</p>
</blockquote>
<script type="math/tex; mode=display">
\begin{aligned}L(\theta)-L(\theta^{(i)}) &= \log \left(\sum_Z P(Y|Z,\theta^{(i)})\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Y|Z,\theta^{(i)})}\right)-\log P(Y|\theta^{(i)}) \\ &\ge \sum_Z P(Z|Y,\theta^{(i)})\log \frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}-\log P(Y|\theta^{(i)}) \\ &= \sum_Z P(Z|Y,\theta^{(i)}) \log \frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}\end{aligned}</script><p>令</p>
<script type="math/tex; mode=display">
B(\theta,\theta^{(i)}) \hat{=} L(\theta^{(i)})+\sum_Z P(Z|Y,\theta^{(i)}) \log \frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})} \ (12.5)</script><p>则</p>
<script type="math/tex; mode=display">
L(\theta) \ge B(\theta,\theta^{(i)}) \ (12.6)</script><p>即<strong>函数<script type="math/tex">B(\theta,\theta^{(i)})</script>是<script type="math/tex">L(\theta)</script>的一个下界</strong>，而且由式(12.5)可知，</p>
<script type="math/tex; mode=display">
L(\theta^{(i)}) = B(\theta^{(i)},\theta^{(i)}) \ (12.7)</script><p>因此，<strong>任何可以使<script type="math/tex">B(\theta,\theta^{(i)})</script>增大的<script type="math/tex">\theta</script>，也可以使<script type="math/tex">L(\theta)</script>增大</strong>，为了使<script type="math/tex">L(\theta)</script>有尽可能大的增长，选择<script type="math/tex">\theta^{(i+1)}</script>使<script type="math/tex">B(\theta,\theta^{(i)})</script>达到极大，即</p>
<script type="math/tex; mode=display">
\theta^{(i+1)} = \arg \max_{\theta} B(\theta,\theta^{(i)}) \ (12.8)</script><p>现在求<script type="math/tex">\theta^{(i+1)}</script>的表达式，省去对<script type="math/tex">\theta</script>的极大化而言是常数的项，由式(12.8)、式(12.5)及式(12.2)，有</p>
<script type="math/tex; mode=display">
\begin{aligned}\theta^{(i+1)}&=\arg \max_{\theta} \left(L(\theta^{(i)}+\sum_Z P(Z|Y,\theta^{(i)})\log \frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}\right) \\ &= \arg \max_{\theta} \left(\sum_Z P(Z|Y,\theta^{(i)})\log(P(Y|Z,\theta)P(Z|\theta))\right)\\ &= \arg \max_{\theta} \left(\sum_Z P(Z|Y,\theta^{(i)}) \log P(Y,Z|\theta)\right) \\ &= \arg \max_{\theta} Q(\theta,\theta^{(i)})\end{aligned} \ (12.9)</script><p>式(12.9)等价于EM算法的一次迭代，即求Q函数及其极大化。<strong>EM算法是通过不断求解下界的极大化逼近求解对数似然函数极大化的算法</strong>。</p>
<p>下图给出EM算法的直观解释。图中上方曲线为<script type="math/tex">L(\theta)</script>，下方曲线为<script type="math/tex">B(\theta,\theta^{(i)})</script>。</p>
<p>根据式(12.6)，<script type="math/tex">B(\theta,\theta^{(i)})</script>为对数似然函数<script type="math/tex">L(\theta)</script>的下界。</p>
<p>根据式(12.7)，两个函数在点<script type="math/tex">\theta=\theta^{(i)}</script>处相等。</p>
<p>根据式(12.8)和式(12.9)，EM算法找到下一个点<script type="math/tex">\theta^{(i+1)}</script>使函数<script type="math/tex">B(\theta,\theta^{(i)})</script>极大化，也使函数<script type="math/tex">Q(\theta,\theta^{(i)})</script>极大化。</p>
<p>由于<script type="math/tex">L(\theta) \ge B(\theta,\theta^{(i)})</script>，函数<script type="math/tex">B(\theta,\theta^{(i)})</script>的增加，保证对数似然函数<script type="math/tex">L(\theta)</script>在每次迭代中也是增加的。</p>
<p>EM算法在点<script type="math/tex">\theta^{(i+1)}</script>重新计算Q函数值，进行下一次迭代。在这个过程中，对数似然函数<script type="math/tex">L(\theta)</script>不断增大。从图可以推断出<strong>EM算法不能保证找到全局最优值</strong>。</p>
<p><img src="image/ch12-1.png" alt="ch12-1"></p>
<h3 id="12-1-3-EM算法在非监督学习中的应用"><a href="#12-1-3-EM算法在非监督学习中的应用" class="headerlink" title="12.1.3 EM算法在非监督学习中的应用"></a>12.1.3 EM算法在非监督学习中的应用</h3><p>当训练数据只有输入没有相应的输出标签，如<script type="math/tex">\{(x_1,\bullet),(x_2,\bullet),\cdots,(x_N,\bullet)\}</script>，从这样的数据学习模型称为<strong>非监督学习问题</strong>。</p>
<p><strong>EM算法可以用于生成模型的非监督学习</strong>。生成模型由联合概率分布P(X,Y)表示，可以认为非监督学习训练数据是联合概率分布产生的数据。X为观测数据，Y为未观测数据。</p>
<h2 id="12-2-EM算法的收敛性"><a href="#12-2-EM算法的收敛性" class="headerlink" title="12.2 EM算法的收敛性"></a>12.2 EM算法的收敛性</h2><p>EM算法提供一种近似计算含有隐变量概率模型的极大似然估计的方法。对于EM算法，需要回答两个问题：<strong>EM算法得到的估计序列是否收敛？如果收敛，是否收敛到全局最大值或局部极大值？</strong></p>
<p>下面给出EM算法收敛性相关的两个定理：</p>
<p><strong>定理12.1</strong> 设<script type="math/tex">P(Y|\theta)</script>为观测数据的似然函数，<script type="math/tex">\theta^{(i)}(i=1,2,\cdots)</script>为EM算法得到的参数估计序列，<script type="math/tex">P(Y|\theta^{(i)})(i=1,2,\cdots)</script>为对应的似然函数序列，则<script type="math/tex">P(Y|\theta^{(i)})</script>是单调递增的，即</p>
<script type="math/tex; mode=display">
P(Y|\theta^{(i+1)}) \ge P(Y|\theta^{(i)}) \ (12.10)</script><p><strong>证明</strong> 由于</p>
<script type="math/tex; mode=display">
P(Y|\theta) = \frac{P(Y,Z|\theta)}{P(Z|Y,\theta)}</script><p>取对数有</p>
<script type="math/tex; mode=display">
\log P(Y|\theta) = \log P(Y,Z|\theta)-\log P(Z|Y,\theta)</script><p>由式(12.3)</p>
<script type="math/tex; mode=display">
Q(\theta,\theta^{(i)}) = \sum_Z \log P(Y,Z|\theta)P(Z|Y,\theta^{(i)})</script><p>令</p>
<script type="math/tex; mode=display">
H(\theta,\theta^{(i)}) = \sum_Z \log P(Z|Y,\theta)P(Z|Y,\theta^{(i)}) \ (12.11)</script><p>于是对数似然函数可以写成</p>
<script type="math/tex; mode=display">
\log P(Y|\theta) = Q(\theta,\theta^{(i)})-H(\theta,\theta^{(i)}) \ (12.12)</script><p>在式(12.12)中分别取<script type="math/tex">\theta</script>为<script type="math/tex">\theta^{(i)}</script>和<script type="math/tex">\theta^{(i+1)}</script>并相减，有</p>
<script type="math/tex; mode=display">
\begin{aligned}\log &P(Y|\theta^{(i+1)})-\log P(Y|\theta^{(i)}) \\&=[Q(\theta^{(i+1)},\theta^{(i)})-Q(\theta^{(i)},\theta^{(i)})]-[H(\theta^{(i+1)},\theta^{(i)})-H(\theta^{(i)},\theta^{(i)})] \end{aligned} \ (12.13)</script><p>为证式(12.10)，只需证式(12.13)右端是非负的。式(12.13)右端的第1项，由于<script type="math/tex">\theta^{(i+1)}</script>使<script type="math/tex">Q(\theta,\theta^{(i)})</script>达到极大，所以有：</p>
<script type="math/tex; mode=display">
Q(\theta^{(i+1)},\theta^{(i)})-Q(\theta^{(i)},\theta^{(i)}) \ge 0 \ (12.14)</script><p>其第2项，由式(12.11)可知:</p>
<script type="math/tex; mode=display">
\begin{aligned}H(\theta^{(i+1)}&,\theta^{(i)})-H(\theta^{(i)},\theta^{(i)}) \\ &= \sum_Z \left(\log \frac{P(Z|Y,\theta^{(i+1)})}{P(Z|Y,\theta^{(i)})}\right)P(Z|Y,\theta^{(i)}) \\ &\le \log \left(\sum_Z  \frac{P(Z|Y,\theta^{(i+1)})}{P(Z|Y,\theta^{(i)})} P(Z|Y,\theta^{(i)})\right) \\ &= \log \left(\sum_Z P(Z|Y,\theta^{(i+1)})\right) = 0\end{aligned} \ (12.15)</script><p><strong>注意:这里的不等号由Jensen不等式得到</strong>。</p>
<p>由式(12.14)和式(12.15)可知式(12.13)右端是非负的。</p>
<p><strong>定理12.2</strong> 设<script type="math/tex">L(\theta)=\log P(Y|\theta)</script>为观测数据的对数似然函数，<script type="math/tex">\theta^{(i)}(i=1,2,\cdots)</script>为EM算法得到的参数估计序列，<script type="math/tex">L(\theta^{(i)})(i=1,2,\cdots)</script>为对应的对数似然函数序列。</p>
<p>(1)如果<script type="math/tex">P(Y|\theta)</script>有上界，则<script type="math/tex">L(\theta^{(i)})=\log P(Y|\theta^{(i)})</script>收敛到某一值<script type="math/tex">L^*</script></p>
<p>(2)在函数<script type="math/tex">Q(\theta,\theta^{'})</script>与<script type="math/tex">L(\theta)</script>满足一定条件下，由EM算法得到的参数估计序列<script type="math/tex">\theta^{(i)}</script>的收敛值<script type="math/tex">\theta^{*}</script>是<script type="math/tex">L(\theta)</script>的稳定点。</p>
<p><strong>证明</strong></p>
<p>(1)由<script type="math/tex">L(\theta)=\log P(Y|\theta^{(i)})</script>的单调性及<script type="math/tex">P(Y|\theta)</script>的有界性立即得到。</p>
<p>(2)证明从略，参考文献:<a href="http://www.cs.toronto.edu/~fritz/absps/emk.pdf" target="_blank" rel="noopener">A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants</a></p>
<p>EM算法的收敛性包含关于对数似然函数序列<script type="math/tex">L(\theta^{(i)})</script>的收敛性和关于参数估计序列<script type="math/tex">\theta^{(i)}</script>的收敛性两层意思。</p>
<p><strong>在应用中，初值的选择变得非常重要，常用的方法是选取几个不同的初值进行迭代，然后对得到的各个估计值加以比较，从中选择最好的</strong>。</p>
<h2 id="12-3-EM算法在高斯混合模型学习中的应用"><a href="#12-3-EM算法在高斯混合模型学习中的应用" class="headerlink" title="12.3 EM算法在高斯混合模型学习中的应用"></a>12.3 EM算法在高斯混合模型学习中的应用</h2><p>在许多情况下，EM算法是学习高斯混合模型的有效方法。</p>
<h3 id="12-3-1-高斯混合模型"><a href="#12-3-1-高斯混合模型" class="headerlink" title="12.3.1 高斯混合模型"></a>12.3.1 高斯混合模型</h3><p><strong>定义12.2(高斯混合模型)</strong> 高斯混合模型是指具有如下形式的概率分布模型:</p>
<script type="math/tex; mode=display">
P(y|\theta) = \sum_{k=1}^K \alpha_k \phi(y|\theta_k) \ (12.16)</script><p>其中，<script type="math/tex">\alpha_k</script>是系数，<script type="math/tex">\alpha_k \ge 0</script>，<script type="math/tex">\sum_{k=1}^K \alpha_k = 1</script>；<script type="math/tex">\phi(y|\theta_k)</script>是高斯分布密度，<script type="math/tex">\theta_k = (\mu_k, \sigma_k^2)</script>，</p>
<script type="math/tex; mode=display">
\phi(y|\theta_k) = \frac{1}{\sqrt{2\pi}\sigma_k} \exp \left(-\frac{(y-\mu_k)^2}{2\sigma_k^2}\right) \ (12.17)</script><p>称为第k个分模型。</p>
<p>一般混合模型可以由任意概率分布密度代替式(12.17)中的高斯分布密度，我们只介绍最常见的<strong>高斯混合模型</strong>。</p>
<h3 id="12-3-2-高斯混合模型参数估计的EM算法"><a href="#12-3-2-高斯混合模型参数估计的EM算法" class="headerlink" title="12.3.2 高斯混合模型参数估计的EM算法"></a>12.3.2 高斯混合模型参数估计的EM算法</h3><p>假设观测数据<script type="math/tex">y_1,y_2,\cdots,y_N</script>由高斯混合模型生成，</p>
<script type="math/tex; mode=display">
P(y|\theta) = \sum_{k=1}^K \alpha_k \phi(y|\theta_k) \ (12.18)</script><p>其中，<script type="math/tex">\theta=(\alpha_1,\alpha_2,\cdots,\alpha_K;\theta_1,\theta_2,\cdots,\theta_K)</script>。我们用EM算法估计高斯混合模型的参数<script type="math/tex">\theta</script>。</p>
<p><strong>1. 明确隐变量，写出完全数据的对数似然函数</strong></p>
<p>观测数据<script type="math/tex">y_j,j=1,2,\cdots,N</script>，是这样产生的:</p>
<ol>
<li><p>首先依概率<script type="math/tex">\alpha_k</script>选择第k个高斯分布分模型<script type="math/tex">\phi(y|\theta_k)</script>；</p>
</li>
<li><p>然后依第k个分模型的概率分布<script type="math/tex">\phi(y|\theta_k)</script>生成观测数据<script type="math/tex">y_j</script>。这时，观测数据<script type="math/tex">y_j,j=1,2,\cdots,N</script>，是已知的；</p>
<p>反映观测数据<script type="math/tex">y_j</script>来自第k个分模型的数据是未知的，k=1,2,…,K，以隐变量<script type="math/tex">\gamma_{jk}</script>表示，其定义如下:</p>
<script type="math/tex; mode=display">
\gamma_{jk}=\left\{\begin{aligned}&1,\mathrm{j-th\ sample \ come \ from \ k-th \ model} \\ &0, \mathrm{others}\end{aligned}\right. \ (12.19)</script><script type="math/tex; mode=display">
j=1,2,\cdots,N;k=1,2,\cdots,K</script><p><script type="math/tex">\gamma_{jk}</script>是0-1随机变量。</p>
</li>
</ol>
<p>有了观测数据<script type="math/tex">y_j</script>及未观测数据<script type="math/tex">\gamma_{jk}</script>，那么完全数据是</p>
<script type="math/tex; mode=display">
(y_j,\gamma_{j1},\gamma_{j2},\cdots,\gamma_{jK}) , j=1,2,\cdots,N</script><p>于是，可以写出完全数据的似然函数:</p>
<script type="math/tex; mode=display">
\begin{aligned}P(y,\gamma|\theta) &= \prod_{j=1}^N P(y_j,\gamma_{j1},\gamma_{j2},\cdots,\gamma_{jK}|\theta) \\ &= \prod_{k=1}^K \prod_{j=1}^N [\alpha_k \phi(y_j|\theta_k)]^{\gamma_{jk}} \\ &= \prod_{k=1}^K \alpha_k^{n_k} \prod_{j=1}^N [\phi(y_j|\theta_k)]^{\gamma_{jk}} \\ &=\prod_{k=1}^K \alpha_k^{n_k} \prod_{j=1}^N \left[\frac{1}{\sqrt{2\pi}\sigma_k} \exp \left(-\frac{(y_j-\mu_k)^2}{2\sigma_k^2}\right)\right]^{\gamma_{jk}}\end{aligned}</script><p>式中，<script type="math/tex">n_k = \sum_{j=1}^N \gamma_{jk}</script>，<script type="math/tex">\sum_{k=1}^K n_k = N</script>。</p>
<p>那么，完全数据的对数似然函数为:</p>
<script type="math/tex; mode=display">
\log P(y,\gamma|\theta) = \sum_{k=1}^K \left\{n_k \log \alpha_k + \sum_{j=1}^N \gamma_{jk} \left[\log \left(\frac{1}{\sqrt{2\pi}}\right)-\log \sigma_k-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\right]\right\}</script><p><strong>2.EM算法的E步:确定Q函数</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}Q(\theta,\theta^{(i)}) &= E[\log P(y,\gamma|\theta)|y,\theta^{(i)}] \\ &= E \left\{\sum_{k=1}^K \left\{n_k \log \alpha_k + \sum_{j=1}^N \gamma_{jk} \left[\log \left(\frac{1}{\sqrt{2\pi}}\right)-\log \sigma_k-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\right]\right\}\right\}\\&=\sum_{k=1}^K\left\{\sum_{j=1}^N(E\gamma_{jk})\log \alpha_k+\sum_{j=1}^N(E\gamma_{jk})\left[\log \left(\frac{1}{\sqrt{2\pi}}\right)-\log \sigma_k-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\right]\right\}\end{aligned} \ (12.20)</script><p>这里需要计算<script type="math/tex">E(\gamma_{jk}|y,\theta)</script>，记为<script type="math/tex">\hat{\gamma}_{jk}</script>。</p>
<script type="math/tex; mode=display">
\begin{aligned}\hat{\gamma}_{jk} &= E(\gamma_{jk}|y,\theta)=P(\gamma_{jk}=1|y,\theta) \\ &= \frac{P(\gamma_{jk}=1,y_j|\theta)}{\sum_{k=1}^K P(\gamma_{jk}=1,y_j|\theta)}\\&=\frac{P(y_j|\gamma_{jk}=1,\theta)P(\gamma_{jk}=1|\theta)}{\sum_{k=1}^K P(y_j|\gamma_{jk}=1,\theta)P(\gamma_jk=1|\theta)}\\&=\frac{\alpha_k \phi(y_j|\theta_k)}{\sum_{k=1}^K \alpha_k \phi(y_j|\theta_k)}\end{aligned}</script><script type="math/tex; mode=display">
j=1,2,\cdots,N;k=1,2,\cdots,K</script><p>关于<script type="math/tex">\hat{\gamma}_{jk}</script>是在当前模型参数下第j个观测数据来自第k个分模型的概率，称为分模型k对观测数据<script type="math/tex">y_j</script>的响应度。</p>
<p>将<script type="math/tex">\hat{\gamma}_{jk}=E\gamma_{jk}</script>及<script type="math/tex">n_k = \sum_{j=1}^N E\gamma_{jk}</script>代入式（12.20）即得</p>
<script type="math/tex; mode=display">
Q(\theta,\theta^{(i)}) = \sum_{k=1}^K \left\{n_k \log \alpha_k + \sum_{j=1}^N \hat{\gamma}_{jk} \left[\log \left(\frac{1}{\sqrt{2\pi}}\right)-\log \sigma_k-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\right]\right\} \ (12.21)</script><p><strong>3. 确定EM算法的M步</strong></p>
<p>迭代的M步是求函数<script type="math/tex">Q(\theta,\theta^{(i)})</script>对<script type="math/tex">\theta</script>的极大值，即求新一轮迭代的模型参数:</p>
<script type="math/tex; mode=display">
\theta^{(i+1)} = \arg \max_{\theta} Q(\theta,\theta^{(i)})</script><p>用<script type="math/tex">\hat{\mu}_k</script>，<script type="math/tex">\hat{\sigma}_k^2</script>及<script type="math/tex">\hat{\alpha}_k</script>，k=1,2,…,K，表示<script type="math/tex">\theta^{(i+1)}</script>的各参数。求<script type="math/tex">\hat{\mu}_k</script>，<script type="math/tex">\hat{\sigma}_k^2</script>只需将式(12.21)分别对<script type="math/tex">\mu_k</script>，<script type="math/tex">\sigma_k^2</script>求偏导数并令其为0，即可得到；求<script type="math/tex">\hat{\alpha}_k</script>是在<script type="math/tex">\sum_{k=1}^K \alpha_k = 1</script>条件下求偏导数并令其为0得到的(<strong>此处使用拉格朗日乘子法求解</strong>)。结果如下：</p>
<script type="math/tex; mode=display">
\hat{\mu}_k = \frac{\sum_{j=1}^N \hat{\gamma}_{jk}y_j}{\sum_{j=1}^N \hat{\gamma}_{jk}},k=1,2,\cdots,K \ (12.22)</script><script type="math/tex; mode=display">
\hat{\sigma}_k^2 = \frac{\sum_{j=1}^N \hat{\gamma}_{jk}(y_j-\mu_k)^2}{\sum_{j=1}^N \hat{\gamma}_{jk}} , k=1,2,\cdots,K \ (12.23)</script><script type="math/tex; mode=display">
\hat{\alpha}_k = \frac{n_k}{N}=\frac{\sum_{j=1}^N \hat{\gamma}_{jk}}{N} , k=1,2,\cdots,K \ (12.24)</script><p>重复以上计算，直到对数似然函数值不再有明显的变化为止。</p>
<p><strong>算法12.2(高斯混合模型参数估计的EM算法)</strong></p>
<p>输入：观测数据<script type="math/tex">y_1,y_2,\cdots,y_N</script>,高斯混合模型</p>
<p>输出：高斯混合模型参数</p>
<p>(1)取参数的初始值开始迭代</p>
<p>(2)E步：依据当前模型参数，计算分模型k对观测数据<script type="math/tex">y_j</script>的响应度</p>
<script type="math/tex; mode=display">
\hat{\gamma}_{jk} = \frac{\alpha_k \phi(y_j|\theta_k)}{\sum_{k=1}^K \alpha_k \phi(y_j|\theta_k)} , j=1,2,\cdots,N;k=1,2,\cdots,K</script><p>(3)M步：计算新一轮迭代的模型参数</p>
<script type="math/tex; mode=display">
\hat{\mu}_k = \frac{\sum_{j=1}^N \hat{\gamma}_{jk}y_j}{\sum_{j=1}^N\hat{\gamma}_{jk}} , k=1,2,\cdots,K</script><script type="math/tex; mode=display">
\hat{\sigma}_k^2 = \frac{\sum_{j=1}^N \hat{\gamma}_{jk}(y_j-\mu_k)^2}{\sum_{j=1}^N \hat{\gamma}_{jk}} , k=1,2,\cdots,K</script><script type="math/tex; mode=display">
\hat{\alpha}_k = \frac{\sum_{j=1}^N \hat{\gamma}_{jk}}{N},k=1,2,\cdots,K</script><p>(4)重复第(2)步和第(3)步，直到收敛。</p>
<h2 id="12-4-EM算法的推广"><a href="#12-4-EM算法的推广" class="headerlink" title="12.4 EM算法的推广"></a>12.4 EM算法的推广</h2><p>EM算法还可以解释为<strong>F函数的极大-极大算法</strong>。</p>
<h3 id="12-4-1-F函数的极大-极大算法"><a href="#12-4-1-F函数的极大-极大算法" class="headerlink" title="12.4.1 F函数的极大-极大算法"></a>12.4.1 F函数的极大-极大算法</h3><p>首先引进F函数并讨论其性质。</p>
<p><strong>定义12.3(F函数)</strong> 假设隐变量数据Z的概率分布为<script type="math/tex">\tilde{P}(Z)</script>，定义分布<script type="math/tex">\tilde{P}</script>与参数<script type="math/tex">\theta</script>的函数<script type="math/tex">F(\tilde{P},\theta)</script>如下：</p>
<script type="math/tex; mode=display">
F(\tilde{P},\theta) = E_{\tilde{P}}[\log P(Y,Z|\theta)]+H(\tilde{P}) \ (12.25)</script><p>称为F函数。式中<script type="math/tex">H(\tilde{P})=-E_{\tilde{P}}\log \tilde{P}(Z)</script>是分布<script type="math/tex">\tilde{P}(Z)</script>的熵。</p>
<blockquote>
<p>通常假设<script type="math/tex">P(Y,Z|\theta)</script>是<script type="math/tex">\theta</script>的连续函数，因而<script type="math/tex">F(\tilde{P},\theta)</script>是<script type="math/tex">\tilde{P}</script>和<script type="math/tex">\theta</script>的连续函数。</p>
</blockquote>
<p><strong>引理12.1</strong> 对于固定的<script type="math/tex">\theta</script>，存在唯一的分布<script type="math/tex">\tilde{P}_{\theta}</script>极大化<script type="math/tex">F(\tilde{P},\theta)</script>，这时<script type="math/tex">\tilde{P}_{\theta}</script>由下式给出:</p>
<script type="math/tex; mode=display">
\tilde{P}_{\theta}(Z) = P(Z|Y,\theta) \ (12.26)</script><p>并且<script type="math/tex">\tilde{P}_{\theta}</script>随<script type="math/tex">\theta</script>连续变化。</p>
<p><strong>证明:</strong> 对于固定的<script type="math/tex">\theta</script>，可以求得使<script type="math/tex">F(\tilde{P},\theta)</script>达到极大的分布<script type="math/tex">\tilde{P}_{\theta}(Z)</script>。为此，引进拉格朗日乘子<script type="math/tex">\lambda</script>，拉格朗日函数为 :</p>
<script type="math/tex; mode=display">
L = E_{\tilde{P}} \log P(Y,Z|\theta)-E_{\tilde{P}}\log \tilde{P}(Z)+\lambda \left(1-\sum_{Z}\tilde{P}(Z)\right) \ (12.27)</script><p>将其对<script type="math/tex">\tilde{P}</script>求偏导数，并令偏导数等于0:</p>
<script type="math/tex; mode=display">
\begin{aligned}\frac{\partial L}{\partial \tilde{P}(Z)} &= \sum_Z \left(\log P(Y,Z|\theta)-\log \tilde{P}(Z) -1 -\lambda\right) \\&= \log P(Y,Z|\theta)-\log \tilde{P}(Z) -1 -\lambda \\ &= 0\end{aligned}</script><p>得出</p>
<script type="math/tex; mode=display">
\lambda = \log P(Y,Z|\theta)-\log \tilde{P}_{\theta}(Z)-1</script><p>由此推出<script type="math/tex">\tilde{P}_{\theta}(Z)</script>与<script type="math/tex">P(Y,Z|\theta)</script>成比例</p>
<script type="math/tex; mode=display">
\frac{P(Y,Z|\theta)}{\tilde{P}_{\theta}} = e^{1+\lambda}</script><p>再从约束条件<script type="math/tex">\sum_Z \tilde{P}_{\theta}=1</script>得式(12.26)(<strong>注意：为什么可以得到式(12.26)这个结论</strong>)</p>
<p>由假设<script type="math/tex">P(Y,Z|\theta)</script>是<script type="math/tex">\theta</script>的连续函数，得到<script type="math/tex">\tilde{P}_{\theta}</script>是<script type="math/tex">\theta</script>的连续函数。</p>
<p><strong>引理12.2</strong> 若<script type="math/tex">\tilde{P}_{\theta}(Z)=P(Z|Y,\theta)</script>，则</p>
<script type="math/tex; mode=display">
F(\tilde{P},\theta) = \log P(Y|\theta) \ (12.28)</script><p>由以上引理，可以得到关于EM算法用F函数的极大-极大算法的解释。</p>
<p><strong>定理12.3</strong> 设<script type="math/tex">L(\theta)=\log P(Y|\theta)</script>为观测数据的对数似然函数，<script type="math/tex">\theta^{(i)}</script>，i=1,2,…,为EM算法得到的参数估计序列，函数<script type="math/tex">F(\tilde{P},\theta)</script>由式(12.25)定义。</p>
<p>如果<script type="math/tex">F(\tilde{P},\theta)</script>在<script type="math/tex">\tilde{P}^*</script>和<script type="math/tex">\theta^{*}</script>有局部极大值，那么<script type="math/tex">L(\theta)</script>也在<script type="math/tex">\theta^{*}</script>有局部极大值。</p>
<p>如果<script type="math/tex">F(\tilde{P},\theta)</script>在<script type="math/tex">\tilde{P}^*</script>和<script type="math/tex">\theta^{*}</script>达到全局最大值，那么<script type="math/tex">L(\theta)</script>也在<script type="math/tex">\theta^{*}</script>达到全局最大值。</p>
<p><strong>证明</strong>：由引理12.1和引理12.2可知，<script type="math/tex">L(\theta)=\log P(Y|\theta)=F(\tilde{P}_{\theta},\theta)</script>对任意<script type="math/tex">\theta</script>成立。特别地，对于使<script type="math/tex">F(\tilde{P},\theta)</script>，有</p>
<script type="math/tex; mode=display">
L(\theta^*) = F(\tilde{P}_{\theta^*},\theta^*)=F(\tilde{P}^*,\theta^*) \ (12.29)</script><p>为了证明<script type="math/tex">\theta^*</script>是<script type="math/tex">L(\theta)</script>的极大点，需要证明不存在接近<script type="math/tex">\theta^*</script>的点<script type="math/tex">\theta^{**}</script>，使<script type="math/tex">L(\theta^{**})>L(\theta^*)</script>。</p>
<p>假如存在这样的点<script type="math/tex">\theta^{**}</script>，那么应有<script type="math/tex">F(\tilde{P}^{**},\theta^{**})>F(\tilde{P}^*,\theta^*)</script>，这里<script type="math/tex">\tilde{P}^{**}=\tilde{P}_{\theta^{**}}</script>。但因<script type="math/tex">\tilde{P}_{\theta}</script>是随<script type="math/tex">\theta</script>连续变化的，<script type="math/tex">\tilde{P}^{**}</script>应接近<script type="math/tex">\tilde{P}^*</script>，这与<script type="math/tex">\tilde{P}^*</script>和<script type="math/tex">\theta^*</script>是<script type="math/tex">F(\tilde{P},\theta)</script>的局部极大点的假设矛盾。</p>
<p><strong>定理12.4</strong> EM算法的一次迭代可由F函数的极大-极大算法实现。</p>
<p>设<script type="math/tex">\theta^{(i)}</script>为第i次迭代参数<script type="math/tex">\theta</script>的估计，<script type="math/tex">\tilde{P}^{(i)}</script>为第i次迭代函数<script type="math/tex">\tilde{P}</script>的估计。在第i+1次迭代的两步为:</p>
<p>(1)对固定的<script type="math/tex">\theta^{(i)}</script>，求<script type="math/tex">\tilde{P}^{(i+1)}</script>使<script type="math/tex">F(\tilde{P},\theta^{(i)})</script>极大化。</p>
<p>(2)对固定的<script type="math/tex">\tilde{P}^{(i+1)}</script>，求<script type="math/tex">\theta^{(i+1)}</script>使<script type="math/tex">F(\tilde{P}^{(i+1)},\theta)</script>极大化</p>
<p><strong>证明</strong></p>
<p>(1)由引理12.1，对于固定的<script type="math/tex">\theta^{(i)}</script>，</p>
<script type="math/tex; mode=display">
\tilde{P}^{(i+1)}(Z) = \tilde{P}_{\theta^{(i)}}(Z) = P(Z|Y,\theta^{(i)})</script><p>使<script type="math/tex">F(\tilde{P},\theta^{(i)})</script>极大化。此时，</p>
<script type="math/tex; mode=display">
\begin{aligned}F(\tilde{P}^{(i+1),\theta}) &= E_{\tilde{P}^{(i+1)}}[\log P(Y,Z|\theta)]+H(\tilde{P}^{(i+1)})\\&=\sum_Z \log P(Y,Z|\theta)P(Z|Y,\theta^{(i)})+H(\tilde{P}^{(i+1)})\end{aligned}</script><p>由<script type="math/tex">Q(\theta,\theta^{(i)})</script>的定义式(12.3)有</p>
<script type="math/tex; mode=display">
F(\tilde{P}^{(i+1)},\theta) = Q(\theta,\theta^{(i)})+H(\tilde{P}^{(i+1)})</script><p>(2)固定<script type="math/tex">\tilde{P}^{(i+1)}</script>，求<script type="math/tex">\theta^{(i+1)}</script>使<script type="math/tex">F(\tilde{P}^{(i+1)},\theta)</script>极大化。得到：</p>
<script type="math/tex; mode=display">
\theta^{(i+1)} = \arg \max_{\theta} F(\tilde{P}^{(i+1)},\theta) = \arg \max_{\theta} Q(\theta,\theta^{(i)})</script><p>通过以上两步完成了EM算法的一次迭代。</p>
<p>由此可知，由EM算法与F函数的极大-极大算法得到的参数估计序列<script type="math/tex">\theta^{(i)}</script>，i=1,2,…,是一致的。</p>
<h3 id="12-4-2-GEM算法"><a href="#12-4-2-GEM算法" class="headerlink" title="12.4.2 GEM算法"></a>12.4.2 GEM算法</h3><p><strong>算法12.3(GEM算法1)</strong></p>
<p>输入：观测数据,F函数</p>
<p>输出：模型参数</p>
<p>(1)初始化参数<script type="math/tex">\theta^{(0)}</script>，开始迭代</p>
<p>(2)第i+1次迭代</p>
<p>第1步:记<script type="math/tex">\theta^{(i)}</script>为参数<script type="math/tex">\theta</script>的估计值，计算</p>
<script type="math/tex; mode=display">
\begin{aligned}Q(\theta,\theta^{(i)})&= E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}]\\&= \sum_Z P(Z|Y,\theta^{(i)})\log P(Y,Z|\theta)\end{aligned}</script><p>(3)第2步:求<script type="math/tex">\theta^{(i+1)}</script>使：</p>
<script type="math/tex; mode=display">
Q(\theta^{(i+1)},\theta^{(i)}) > Q(\theta^{(i)},\theta^{(i)})</script><p>(4)重复(2)和(3)，直到收敛。</p>
<p>当参数<script type="math/tex">\theta</script>的维数为d<script type="math/tex">(d \ge 2)</script>时，可采用一种特殊的GEM算法，它将EM算法的M步分解为d次条件极大化，每次只改变参数向量的一个分量，其余分量不改变。</p>
<p><strong>算法9.5(GEM算法3)</strong></p>
<p>输入:观测数据，Q函数</p>
<p>输出:模型参数</p>
<p>(1) 初始化参数<script type="math/tex">\theta^{(0)}=(\theta_1^{(0)},\theta_2^{(0)},\cdots,\theta_d^{(0)})</script>，开始迭代</p>
<p>(2)第i+1次迭代，</p>
<p>第1步：记<script type="math/tex">\theta^{(i)}=(\theta_1^{(i)},\theta_2^{(i)},\cdots,\theta_d^{(i)})</script>为参数<script type="math/tex">\theta=(\theta_1,\theta_2,\cdots,\theta_d)</script>的估计值，计算</p>
<script type="math/tex; mode=display">
\begin{aligned}Q(\theta,\theta^{(i)}) &= E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}] \\ &= \sum_ZP(Z|y,\theta^{(i)})\log P(Y,Z|\theta)\end{aligned}</script><p>(3)第2步:进行d次条件极大化:</p>
<p>首先，在<script type="math/tex">\theta_2^{(i)},\cdots,\theta_k^{(i)}</script>保持不变的条件下求使<script type="math/tex">Q(\theta,\theta^{(i)})</script>达到极大的<script type="math/tex">\theta_1^{(i+1)}</script></p>
<p>然后，在<script type="math/tex">\theta_1=\theta_1^{(i+1)}</script>，<script type="math/tex">\theta_j=\theta_j^{(i)}</script>，j=3,4,…,k的条件下求使<script type="math/tex">Q(\theta,\theta^{(i)})</script>达到极大的<script type="math/tex">\theta_2^{(i+1)}</script>；</p>
<p>如此继续，经过d次条件极大化，得到<script type="math/tex">\theta^{(i+1)}=(\theta_1^{(i+1)},\theta_2^{(i+1)},\cdots,\theta_d^{(i+1)})</script>使得</p>
<script type="math/tex; mode=display">
Q(\theta^{(i+1)},\theta^{(i)}) > Q(\theta^{(i)},\theta^{(i)})</script><p>(4)重复(2)和(3)。直到收敛。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/学习算法/" rel="tag"># 学习算法</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/06/14/提升树/" rel="next" title="提升树">
                <i class="fa fa-chevron-left"></i> 提升树
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/06/14/隐马尔科夫模型/" rel="prev" title="隐马尔科夫模型">
                隐马尔科夫模型 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">jozee</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">25</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">30</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#12-1-EM算法的引入"><span class="nav-number">1.</span> <span class="nav-text">12.1 EM算法的引入</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#12-1-1-EM算法"><span class="nav-number">1.1.</span> <span class="nav-text">12.1.1 EM算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-1-2-EM算法的导出"><span class="nav-number">1.2.</span> <span class="nav-text">12.1.2 EM算法的导出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-1-3-EM算法在非监督学习中的应用"><span class="nav-number">1.3.</span> <span class="nav-text">12.1.3 EM算法在非监督学习中的应用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-2-EM算法的收敛性"><span class="nav-number">2.</span> <span class="nav-text">12.2 EM算法的收敛性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-3-EM算法在高斯混合模型学习中的应用"><span class="nav-number">3.</span> <span class="nav-text">12.3 EM算法在高斯混合模型学习中的应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#12-3-1-高斯混合模型"><span class="nav-number">3.1.</span> <span class="nav-text">12.3.1 高斯混合模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-3-2-高斯混合模型参数估计的EM算法"><span class="nav-number">3.2.</span> <span class="nav-text">12.3.2 高斯混合模型参数估计的EM算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-4-EM算法的推广"><span class="nav-number">4.</span> <span class="nav-text">12.4 EM算法的推广</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#12-4-1-F函数的极大-极大算法"><span class="nav-number">4.1.</span> <span class="nav-text">12.4.1 F函数的极大-极大算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-4-2-GEM算法"><span class="nav-number">4.2.</span> <span class="nav-text">12.4.2 GEM算法</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">jozee</span>

  
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
    </script>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_pv">
        本站总访问量: <span id="busuanzi_value_site_pv"></span>次
    </span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">
        本站访客数<span id="busuanzi_value_site_uv"></span>人次
    </span>
</div>









        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://jozeelin.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2019/06/14/EM算法及其推广/';
          this.page.identifier = '2019/06/14/EM算法及其推广/';
          this.page.title = 'EM算法及其推广';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://jozeelin.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
