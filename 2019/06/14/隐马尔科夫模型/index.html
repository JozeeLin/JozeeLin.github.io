<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="生成模型,标注模型,">










<meta name="description" content="隐马尔科夫模型(HMM)是可用于标注问题的统计学习模型，描述由隐藏的马尔科夫链随机生成观测序列的过程，属于生成模型。 本章首先介绍隐马尔可夫模型的基本概念，然后分别叙述隐马尔可夫模型的概率计算算法、学习算法及其预测算法。 隐马尔可夫模型在语音识别、自然语言处理、生物信息、模式识别等领域有着广泛的应用。 13.1 隐马尔科夫模型的基本概念13.1.1隐马尔科夫模型的定义定义13.1(隐马尔科夫模型)">
<meta name="keywords" content="生成模型,标注模型">
<meta property="og:type" content="article">
<meta property="og:title" content="隐马尔科夫模型">
<meta property="og:url" content="http://yoursite.com/2019/06/14/隐马尔科夫模型/index.html">
<meta property="og:site_name" content="Jozee&#39;s技术博客">
<meta property="og:description" content="隐马尔科夫模型(HMM)是可用于标注问题的统计学习模型，描述由隐藏的马尔科夫链随机生成观测序列的过程，属于生成模型。 本章首先介绍隐马尔可夫模型的基本概念，然后分别叙述隐马尔可夫模型的概率计算算法、学习算法及其预测算法。 隐马尔可夫模型在语音识别、自然语言处理、生物信息、模式识别等领域有着广泛的应用。 13.1 隐马尔科夫模型的基本概念13.1.1隐马尔科夫模型的定义定义13.1(隐马尔科夫模型)">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/06/14/隐马尔科夫模型/image/ch13-1.png">
<meta property="og:image" content="http://yoursite.com/2019/06/14/隐马尔科夫模型/image/ch13-2.png">
<meta property="og:image" content="http://yoursite.com/2019/06/14/隐马尔科夫模型/image/ch13-3.png">
<meta property="og:updated_time" content="2019-06-14T09:47:47.859Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="隐马尔科夫模型">
<meta name="twitter:description" content="隐马尔科夫模型(HMM)是可用于标注问题的统计学习模型，描述由隐藏的马尔科夫链随机生成观测序列的过程，属于生成模型。 本章首先介绍隐马尔可夫模型的基本概念，然后分别叙述隐马尔可夫模型的概率计算算法、学习算法及其预测算法。 隐马尔可夫模型在语音识别、自然语言处理、生物信息、模式识别等领域有着广泛的应用。 13.1 隐马尔科夫模型的基本概念13.1.1隐马尔科夫模型的定义定义13.1(隐马尔科夫模型)">
<meta name="twitter:image" content="http://yoursite.com/2019/06/14/隐马尔科夫模型/image/ch13-1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/06/14/隐马尔科夫模型/">





  <title>隐马尔科夫模型 | Jozee's技术博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jozee's技术博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/14/隐马尔科夫模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jozee">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jozee's技术博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">隐马尔科夫模型</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-14T17:41:00+08:00">
                2019-06-14
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2019/06/14/隐马尔科夫模型/" class="leancloud_visitors" data-flag-title="隐马尔科夫模型">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>隐马尔科夫模型(HMM)是<strong>可用于标注问题</strong>的统计学习模型，<strong>描述由隐藏的马尔科夫链随机生成观测序列的过程</strong>，属于<strong>生成模型</strong>。</p>
<p>本章首先介绍隐马尔可夫模型的基本概念，然后分别叙述隐马尔可夫模型的概率计算算法、学习算法及其预测算法。</p>
<p>隐马尔可夫模型在语音识别、自然语言处理、生物信息、模式识别等领域有着广泛的应用。</p>
<h2 id="13-1-隐马尔科夫模型的基本概念"><a href="#13-1-隐马尔科夫模型的基本概念" class="headerlink" title="13.1 隐马尔科夫模型的基本概念"></a>13.1 隐马尔科夫模型的基本概念</h2><h3 id="13-1-1隐马尔科夫模型的定义"><a href="#13-1-1隐马尔科夫模型的定义" class="headerlink" title="13.1.1隐马尔科夫模型的定义"></a>13.1.1隐马尔科夫模型的定义</h3><p><strong>定义13.1(隐马尔科夫模型)</strong> 隐马尔可夫模型是<strong>关于时序的概率模型</strong>，描述由一个隐藏的马尔科夫链随机(首先)<strong>生成不可观测的状态随机序列</strong>，再由各个状态生成一个观测而(然后)<strong>产生观测随机序列的过程</strong>。</p>
<p>隐藏的马尔科夫链随机生成的状态的序列，称为<strong>状态序列</strong>；每个状态生成一个观测，而由此产生的观测的随机序列，称为<strong>观测序列</strong>。序列的每一个位置又可以看作是一个时刻。</p>
<p>隐马尔可夫模型由<strong>初始概率分布</strong>、<strong>状态转移概率分布</strong>以及<strong>观测概率分布</strong>确定。隐马尔可夫模型的形式定义如下:</p>
<p>设Q是所有可能的状态的集合，V是所有可能的观测的集合。</p>
<script type="math/tex; mode=display">
Q=\{q_1,q_2,\cdots,q_N\},V=\{v_1,v_2,\cdots,v_M\}</script><p>其中，N是可能的状态数，M是可能的观测数。</p>
<p>I是长度为T的状态序列，O是对应的观测序列。</p>
<script type="math/tex; mode=display">
I=(i_1,i_2,\cdots,i_T),O=(o_1,o_2,\cdots,o_T)</script><p>A是状态转移概率矩阵:</p>
<script type="math/tex; mode=display">
A = [a_{ij}]_{N\times N} \ (13.1)</script><p>其中，</p>
<script type="math/tex; mode=display">
a_{ij} = P(i_{t+1}=q_j|i_t=q_i),i=1,2,\cdots,N;j=1,2,\cdots,N \ (13.2)</script><p>是在时刻t处于状态<script type="math/tex">q_i</script>的条件下在时刻t+1转移到状态<script type="math/tex">q_j</script>的概率。</p>
<p>B是观测概率矩阵:</p>
<script type="math/tex; mode=display">
B = [b_j(k)]_{N\times N} \ (13.3)</script><p>其中，</p>
<script type="math/tex; mode=display">
b_j(k) = P(o_t=v_k|i_t=q_j),k=1,2,\cdots,M;j=1,2,\cdots,N \ (13.4)</script><p>是在时刻t处于状态<script type="math/tex">q_j</script>的条件下生成观测<script type="math/tex">v_k</script>的概率。</p>
<p>关于<script type="math/tex">\pi</script>是初始状态概率向量:</p>
<script type="math/tex; mode=display">
\pi = (\pi_i) \ (13.5)</script><p>其中，</p>
<script type="math/tex; mode=display">
\pi_i = P(i_1=q_i),i=1,2,\cdots,N \ (13.6)</script><p>是时刻t=1处于状态<script type="math/tex">q_i</script>的概率。</p>
<p>隐马尔科夫模型是由初始状态概率向量<script type="math/tex">\pi</script>、状态转移概率矩阵A和观测概率矩阵B决定。<script type="math/tex">\pi</script>和A决定状态序列，B决定观测序列。因此，隐马尔科夫模型<script type="math/tex">\lambda</script>可以用三元符号表示，即</p>
<script type="math/tex; mode=display">
\lambda = (A,B,\pi) \ (13.7)</script><p>A,B,<script type="math/tex">\pi</script>称为隐马尔可夫模型的三要素。</p>
<p>状态转移概率矩阵矩阵A与初始状态概率向量<script type="math/tex">\pi</script>确定了隐藏的马尔科夫链，生成不可观测的状态序列。</p>
<p>观测概率矩阵B确定了如何从状态生成观测，与状态序列综合确定了如何产生观测序列。</p>
<p>从定义可知，隐马尔科夫模型作了<strong>两个基本假设</strong>：</p>
<p><strong>(1)齐次马尔科夫性假设</strong>，即假设隐藏的马尔科夫链在任意时刻t的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻t无关。</p>
<script type="math/tex; mode=display">
P(i_t|i_{t-1},o_{t-1},\cdots,i_1,o_1)=P(i_t|i_{t-1}),t=1,2,\cdots,T \ (13.8)</script><p><strong>(2)观测独立性假设</strong>，即假设任意时刻的观测只依赖于该时刻的马尔科夫链的状态，与其他观测及状态无关。</p>
<script type="math/tex; mode=display">
P(o_t|i_{t-1},o_{t-1},\cdots,i_1,o_1)=P(o_t|i_t),t=1,2,\cdots,T \ (13.9)</script><blockquote>
<p>隐马尔可夫模型可以用于标注，这时状态对应着标记。标记问题是给定观测的序列，预测其对应的标记序列。</p>
</blockquote>
<h3 id="13-1-2-观测序列的生成过程"><a href="#13-1-2-观测序列的生成过程" class="headerlink" title="13.1.2 观测序列的生成过程"></a>13.1.2 观测序列的生成过程</h3><p>根据隐马尔可夫模型定义，可以将一个长度为T的观测序列<script type="math/tex">O=(o_1,o_2,\cdots,o_T)</script>的生成过程描述如下:</p>
<p><strong>算法10.1(观测序列的生成)</strong></p>
<p>输入:隐马尔可夫模型<script type="math/tex">\lambda=(A,B,\pi)</script>，观测序列长度T；</p>
<p>输出:观测序列<script type="math/tex">O=(o_1,o_2,\cdots,o_T)</script>。</p>
<p>(1)按照初始状态分布<script type="math/tex">\pi</script>产生状态<script type="math/tex">i_1</script></p>
<p>(2)令t=1</p>
<p>(3)按照状态<script type="math/tex">i_t</script>的观测概率分布<script type="math/tex">b_{i_t}(k)</script>生成<script type="math/tex">o_t</script></p>
<p>(4)按照状态<script type="math/tex">i_t</script>的状态转移概率分布<script type="math/tex">\{a_{i_ti_{t+1}}\}</script>产生状态<script type="math/tex">i_{t+1}</script>，<script type="math/tex">i_{t+1}=1,2,\cdots,N</script></p>
<p>(5)令t=t+1;如果t&lt;T，转步(3)；否则，终止</p>
<h3 id="13-1-3-隐马尔科夫模型的3个基本问题"><a href="#13-1-3-隐马尔科夫模型的3个基本问题" class="headerlink" title="13.1.3 隐马尔科夫模型的3个基本问题"></a>13.1.3 隐马尔科夫模型的3个基本问题</h3><p>隐马尔可夫模型有3个基本问题:</p>
<p><strong>(1)概率计算问题</strong>。给定模型<script type="math/tex">\lambda=(A,B,\pi)</script>和观测序列<script type="math/tex">O=(o_1,o_2,\cdots,o_T)</script>，计算在模型<script type="math/tex">\lambda</script>下观测序列O出现的概率<script type="math/tex">P(O|\lambda)</script>。</p>
<p><strong>(2)学习问题</strong>。已知观测序列<script type="math/tex">O=(o_1,o_2,\cdots,o_T)</script>，估计模型<script type="math/tex">\lambda=(A,B,\pi)</script>参数，使得在该模型下观测序列概率<script type="math/tex">P(O|\lambda)</script>最大。即用极大似然估计的方法估计参数。</p>
<p><strong>(3)预测问题</strong>。也称为解码问题。已知模型<script type="math/tex">\lambda=(A,B,\pi)</script>和观测序列<script type="math/tex">O=(o_1,o_2,\cdots,o_T)</script>，求对给定观测序列条件概率<script type="math/tex">P(I|O)</script>最大的状态序列<script type="math/tex">I=(i_1,i_2,\cdots,i_T)</script>。即<strong>给定观测序列，求最有可能的对应的状态序列</strong>。</p>
<h2 id="13-2-概率计算算法"><a href="#13-2-概率计算算法" class="headerlink" title="13.2 概率计算算法"></a>13.2 概率计算算法</h2><p>本节介绍计算观测序列概率<script type="math/tex">P(O|\lambda)</script>的前向(forward)和后向(backward)算法。先介绍概念上可行但计算上不可行的直接计算法。</p>
<h3 id="13-2-1-直接计算法"><a href="#13-2-1-直接计算法" class="headerlink" title="13.2.1 直接计算法"></a>13.2.1 直接计算法</h3><p>给定模型<script type="math/tex">\lambda=(A,B,\pi)</script>和观测序列<script type="math/tex">O=(o_1,o_2,\cdots,o_T)</script>，计算观测序列O出现的概率<script type="math/tex">P(O|\lambda)</script>。最直接的方法是按概率公式直接计算。通过列举所有可能的长度为T的状态序列<script type="math/tex">I=(i_1,i_2,\cdots,i_T)</script>，<strong>求各个状态序列I与观测序列<script type="math/tex">O=(o_1,o_2,\cdots,o_T)</script>的联合概率<script type="math/tex">P(O,I|\lambda)</script>，然后对所有可能的状态序列求和，得到<script type="math/tex">P(O|\lambda)</script></strong>。</p>
<p>状态序列<script type="math/tex">I=(i_1,i_2,\cdots,i_T)</script>的概率是</p>
<script type="math/tex; mode=display">
P(I|\lambda) = \pi_{i_1}a_{i_1i_2}a_{i_2i_3}\cdots a_{i_{T-1}i_T} \ (13.10)</script><p>对固定的状态序列<script type="math/tex">I=(i_1,i_2,\cdots,i_T)</script>，观测序列<script type="math/tex">O=(o_1,o_2,\cdots,o_T)</script>的概率是<script type="math/tex">P(O|I,\lambda)</script>，</p>
<script type="math/tex; mode=display">
P(O|I,\lambda) = b_{i_1}(o_1)b_{i_2}(o_2)\cdots b_{i_T}(o_T) \ (13.11)</script><p>O和I同时出现的联合概率为:</p>
<script type="math/tex; mode=display">
\begin{aligned}P(O,I|\lambda)&=P(O|I,\lambda)P(I|\lambda)\\&=\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)\cdots a_{i_{T-1}i_T}b_{i_T}(o_T) \end{aligned} \ (13.12)</script><p>然后，对所有可能的状态序列I求和，得到观测序列O的概率<script type="math/tex">P(O|\lambda)</script>，即</p>
<script type="math/tex; mode=display">
\begin{aligned}P(O|\lambda)&=\sum_I P(O|I,\lambda)P(I|\lambda) \\ &= \sum_{i_1,i_2,\cdots,i_T} \pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)\cdots a_{i_{T-1}i_T}b_{i_T}(o_T)\end{aligned} \ (13.13)</script><p>但是，利用公式(13.13)计算量很大，是<script type="math/tex">O(TN^T)</script>阶的，这种算法不可行。</p>
<h3 id="13-2-2-前向算法"><a href="#13-2-2-前向算法" class="headerlink" title="13.2.2 前向算法"></a>13.2.2 前向算法</h3><p><strong>定义13.2(前向概率)</strong> 给定隐马尔可夫模型<script type="math/tex">\lambda</script>，定义到时刻t部分观测序列为<script type="math/tex">o_1,o_2,\cdots,o_t</script>且状态为<script type="math/tex">q_i</script>的概率为前向概率，记作</p>
<script type="math/tex; mode=display">
\alpha_t(i) = P(o_1,o_2,\cdots,o_t,i_t=q_i|\lambda) \ (13.14)</script><p>可以递推的求得前向概率<script type="math/tex">\alpha_t(i)</script>及观测序列概率<script type="math/tex">P(O|\lambda)</script>。</p>
<p><strong>算法13.2(观测序列概率的前向算法)</strong></p>
<p>输入：隐马尔可夫模型<script type="math/tex">\lambda</script>，观测序列O;</p>
<p>输出：观测序列概率<script type="math/tex">P(O|\lambda)</script></p>
<p>(1)初值</p>
<script type="math/tex; mode=display">
\alpha_1(i) = \pi_i b_i(o_1),i=1,2,\cdots,N \ (13.15)</script><p>(2)递推 对t=1,2,…,T-1，</p>
<script type="math/tex; mode=display">
\alpha_{t+1}(i) = \left[\sum_{j=1}^N \alpha_t(j)a_{ji}\right] b_i(o_{t+1}),i=1,2,\cdots,N (13.16)</script><p>(3)终止</p>
<script type="math/tex; mode=display">
P(O|\lambda) = \sum_{i=1}^N \alpha_T(i) \ (13.17)</script><p>前向算法解释如下:</p>
<p>步骤(1)初始化前向概率，是初始时刻的状态<script type="math/tex">i_1=q_i</script>和观测<script type="math/tex">o_1</script>的联合概率。</p>
<p>步骤(2)是前向概率的递推公式，计算到时刻t+1部分观测序列为<script type="math/tex">o_1,o_2,\cdots,o_t,o_{t+1}</script>且在时刻t+1处于状态<script type="math/tex">q_i</script>的前向概率。</p>
<p>如下图所示。在式(13.16)的方括弧里，既然<script type="math/tex">\alpha_t(j)</script>是到时刻t观测到<script type="math/tex">o_1,o_2,\cdots,o_t</script>并在时刻t处于状态<script type="math/tex">q_j</script>的前向概率，那么乘积<script type="math/tex">\alpha_t(j)a_{ji}</script>就是时刻t观测到<script type="math/tex">o_1,o_2,\cdots,o_t</script>并在时刻t处于状态<script type="math/tex">q_j</script>而在时刻t-1到达状态<script type="math/tex">q_i</script>的联合概率。</p>
<p>对这个乘积在时刻t的所有可能的N个状态<script type="math/tex">q_j</script>求和，其结果就是到时刻t观测为<script type="math/tex">o_1,o_2,\cdots,o_t</script>并在时刻t+1处于状态<script type="math/tex">q_i</script>的联合概率。</p>
<p>方括号里的值与观测概率<script type="math/tex">b_i(o_{t+1})</script>的乘积恰好是到时刻t+1观测到<script type="math/tex">o_1,o_2,\cdots,o_t,o_{t+1}</script>并在时刻t+1处于状态<script type="math/tex">q_i</script>的前向概率<script type="math/tex">\alpha_{t+1}(i)</script>。</p>
<p><img src="image/ch13-1.png" alt="ch13-1"></p>
<p>步骤(3)给出<script type="math/tex">P(O|\lambda)</script>的计算公式。因为</p>
<script type="math/tex; mode=display">
\alpha_T(i) = P(o_1,o_2,\cdots,o_T,i_T=q_i|\lambda)</script><p>所以</p>
<script type="math/tex; mode=display">
P(O|\lambda) = \sum_{i=1}^N \alpha_T(i)</script><p>如下图所示，<strong>前向算法实际上是基于”状态序列的路径结构”递推计算<script type="math/tex">P(O|\lambda)</script>的算法</strong>。前向算法高效的关键是其<strong>局部计算前向概率</strong>，然后利用路径结构将前向概率”递推”到全局，得到<script type="math/tex">P(O|\lambda)</script>。</p>
<p>具体地，在时刻t=1，计算<script type="math/tex">\alpha_1(i)</script>的N个值(i=1,2,…,N)；在各个时刻t=1,2,…,T-1，计算<script type="math/tex">\alpha_{t+1}(i)</script>的N个值(i=1,2,…,N)，而且每个<script type="math/tex">\alpha_{t+1}(i)</script>的计算利用前一时刻N个<script type="math/tex">\alpha_t(j)</script>。<strong>减少计算量的原因在于每一次计算直接引用前一个时刻的计算结果，避免重复计算</strong>。这样，利用前向概率计算<script type="math/tex">P(O|\lambda)</script>的计算量是<script type="math/tex">O(N^2T)</script>阶的，而不是直接计算的<script type="math/tex">O(TN^T)</script>阶。</p>
<p><img src="image/ch13-2.png" alt="ch13-2"></p>
<h3 id="13-2-3-后向算法"><a href="#13-2-3-后向算法" class="headerlink" title="13.2.3 后向算法"></a>13.2.3 后向算法</h3><p><strong>定义13.3(后向概率)</strong> 给定隐马尔可夫模型<script type="math/tex">\lambda</script>，定义在时刻t状态为<script type="math/tex">q_i</script>的条件下，从t+1到T的部分观测序列为<script type="math/tex">o_{t+1},o_{t+2},\cdots,o_T</script>的概率为后向概率，记作</p>
<script type="math/tex; mode=display">
\beta_t(i) = P(o_{t+1},o_{t+2},\cdots,o_T|i_t=q_i,\lambda)\ (13.18)</script><p>可以用递推的方法求得后向概率<script type="math/tex">\beta_t(i)</script>及观测序列概率<script type="math/tex">P(O|\lambda)</script>。</p>
<p><strong>算法13.3(观测序列概率的后向算法)</strong></p>
<p>输入:隐马尔可夫模型<script type="math/tex">\lambda</script>，观测序列O</p>
<p>输出:观测序列概率<script type="math/tex">P(O|\lambda)</script></p>
<p>(1)</p>
<script type="math/tex; mode=display">
\beta_T(i)=1,i=1,2,\cdots,N \ (13.19)</script><p>(2)对t=T-1,T-2,…,1</p>
<script type="math/tex; mode=display">
\beta_t(i) = \sum_{j=1}^N a_{ij}b_j(o_{t+1})\beta_{t+1}(j) , i=1,2,\cdots,N \ (13.20)</script><p>(3)</p>
<script type="math/tex; mode=display">
P(O|\lambda) = \sum_{i=1}^N \pi_i b_i(o_1)\beta_1(i) \ (13.21)</script><p>算法解释如下：</p>
<p>步骤(1)初始化后向概率，对最终时刻的所有状态<script type="math/tex">q_i</script>规定<script type="math/tex">\beta_T(i)=1</script>。</p>
<p>步骤(2)是后向概率的递推公式。</p>
<p>如下图所示，</p>
<p><img src="image/ch13-3.png" alt="ch13-3"></p>
<p>为了计算在时刻t状态为<script type="math/tex">q_i</script>条件下时刻t+1之后的观测序列为<script type="math/tex">o_{t+1},o_{t+2},\cdots,o_T</script>的后向概率<script type="math/tex">\beta_t(i)</script>，只需考虑在时刻t+1所有可能的N个状态<script type="math/tex">q_j</script>的转移概率(即<script type="math/tex">\alpha_{ij}</script>项)，以及在此状态下的观测<script type="math/tex">o_{t+1}</script>的观测概率(即<script type="math/tex">b_j(o_{t+1})</script>项)，然后考虑状态<script type="math/tex">q_j</script>之后的观测序列的后向概率(即<script type="math/tex">\beta_{t+1}(j)</script>项)。</p>
<p>步骤(3)求<script type="math/tex">P(O|\lambda)</script>的思路与步骤(2)一致，只是初始概率<script type="math/tex">\pi_i</script>代替转移概率。</p>
<p>利用前向概率和后向概率的定义可以将观测序列概率<script type="math/tex">P(O|\lambda)</script>统一写成</p>
<script type="math/tex; mode=display">
P(O|\lambda) = \sum_{i=1}^N\sum_{j=1}^N \alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j),t=1,2,\cdots,T-1 \ (13.22)</script><p>此式当t=1和t=T-1时分别为式(13.21)和式(13.17)。</p>
<h3 id="13-2-4-一些概率与期望值的计算"><a href="#13-2-4-一些概率与期望值的计算" class="headerlink" title="13.2.4 一些概率与期望值的计算"></a>13.2.4 一些概率与期望值的计算</h3><p>利用前向概率和后向概率，可以得到关于单个状态和两个状态概率的计算公式。</p>
<ol>
<li><p>给定模型<script type="math/tex">\lambda</script>和观测O，在时刻t处于状态<script type="math/tex">q_i</script>的概率。记</p>
<script type="math/tex; mode=display">
\gamma_t(i)=P(i_t=q_i|O,\lambda) (13.23)</script><p>可以通过前向后向概率计算。事实上，</p>
<script type="math/tex; mode=display">
\gamma_t(i)=P(i_t=q_i|O,\lambda)=\frac{P(i_t=q_i,O|\lambda)}{P(O|\lambda)}</script><p>由前向概率<script type="math/tex">\alpha_t(i)</script>和后向概率<script type="math/tex">\beta_t(i)</script>定义可知:</p>
<script type="math/tex; mode=display">
\alpha_t(i)\beta_t(i) = P(i_t=q_i,O|\lambda)</script><p>于是得到:</p>
<script type="math/tex; mode=display">
\gamma_t(i)=\frac{\alpha_t(i)\beta_t(i)}{P(O|\lambda)}=\frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N \alpha_t(j)\beta_t(j)} \ (13.24)</script></li>
<li><p>给定模型<script type="math/tex">\lambda</script>和观测O，在时刻t处于状态<script type="math/tex">q_i</script>且在时刻t+1处于状态<script type="math/tex">q_j</script>的概率。记</p>
<script type="math/tex; mode=display">
\xi_t(i,j) = P(i_t=q_i,i_{t+1}=q_j|O,\lambda) \ (13.25)</script><p>可以通过前向后向概率计算:</p>
<script type="math/tex; mode=display">
\xi_t(i,j)=\frac{P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}{P(O|\lambda)} = \frac{P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}{\sum_{i=1}^N\sum_{j=1}^N P(i_t=q_i,i_{i+1}=q_j,O|\lambda)}</script><p>而</p>
<script type="math/tex; mode=display">
P(i_t=q_i,i_{t+1}=q_j,O|\lambda)=\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)</script><p>所以</p>
<script type="math/tex; mode=display">
\xi_t(i,j)=\frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^N\sum_{j=1}^N \alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)} \ (13.26)</script></li>
<li><p>将<script type="math/tex">\gamma_t(i)</script>和<script type="math/tex">\xi_t(i,j)</script>对各个时刻t求和，可以得到一些有用的期望值:</p>
<ol>
<li><p>在观测O下状态i出现的期望值:</p>
<script type="math/tex; mode=display">
\sum_{t=1}^T \gamma_t(i) \ (13.27)</script></li>
<li><p>在观测O下由状态i转移的期望值</p>
<script type="math/tex; mode=display">
\sum_{t=1}^{T-1}\gamma_t(i) \ (13.28)</script></li>
<li><p>在观测O下由状态i转移到状态j的期望值</p>
<script type="math/tex; mode=display">
\sum_{t=1}^{T-1}\xi_t(i,j) \ (13.29)</script></li>
</ol>
</li>
</ol>
<h2 id="13-3-学习算法"><a href="#13-3-学习算法" class="headerlink" title="13.3 学习算法"></a>13.3 学习算法</h2><p>隐马尔可夫模型的学习，<strong>根据训练数据是包括观测序列和对应的状态序列还是只有观测序列，可以分别由监督学习与非监督学习实现</strong>。</p>
<p>本节首<strong>先介绍监督学习算法</strong>，而<strong>后介绍非监督学习算法</strong>——Baum-Welch算法(也就是EM算法)。</p>
<h3 id="13-3-1-监督学习算法"><a href="#13-3-1-监督学习算法" class="headerlink" title="13.3.1 监督学习算法"></a>13.3.1 监督学习算法</h3><p>假设已给训练数据包含S个长度相同的观测序列和对应的状态序列<script type="math/tex">\{(O_1,I_1),(O_2,I_2),\cdots,(O_S,I_S)\}</script>，那么可以利用极大似然估计法来估计隐马尔可夫模型的参数。具体方法如下:</p>
<p><strong>1.转移概率<script type="math/tex">a_{ij}</script>的估计</strong></p>
<p>设样本中时刻t处于状态i时刻t+1转移到状态j的频数为<script type="math/tex">A_{ij}</script>，那么状态转移概率<script type="math/tex">a_{ij}</script>的估计是：</p>
<script type="math/tex; mode=display">
\hat{a}_{ij} = \frac{A_{ij}}{\sum_{j=1}^N A_{ij}},i=1,2,\cdots,N;j=1,2,\cdots,N \ (13.30)</script><p><strong>2.观测概率<script type="math/tex">b_j(k)</script>的估计</strong></p>
<p>设样本中状态为j并观测为k的频数是<script type="math/tex">B_{jk}</script>，那么状态为j观测为k的概率<script type="math/tex">b_j(k)</script>的估计是</p>
<script type="math/tex; mode=display">
\hat{b}_j(k) = \frac{B_{jk}}{\sum_{k=1}^M B_{jk}},j=1,2,\cdots,N;k=1,2,\cdots,M \ (13.31)</script><p><strong>3.初始状态概率<script type="math/tex">\pi_i</script>的估计<script type="math/tex">\hat{\pi}_i</script>为S个样本中初始状态为<script type="math/tex">q_i</script>的频率</strong></p>
<p>由于监督学习需要使用训练数据，而人工标注训练数据往往代价很高，有时就会利用非监督学习的方法。</p>
<h3 id="13-3-2-Baum-Welch算法"><a href="#13-3-2-Baum-Welch算法" class="headerlink" title="13.3.2 Baum-Welch算法"></a>13.3.2 Baum-Welch算法</h3><p>假设给定训练数据只包含S个长度为T的观测序列<script type="math/tex">\{O_1,O_2,\cdots,O_S\}</script>而没有对应的状态序列，目标是学习隐马尔可夫模型<script type="math/tex">\lambda=(A,B,\pi)</script>的参数。</p>
<p>我们将观测序列数据看作观测数据O，状态序列数据看作不可观测的隐数据I，那么隐马尔可夫模型事实上是一个含有隐变量的概率模型。</p>
<script type="math/tex; mode=display">
P(O|\lambda) = \sum_I P(O|I,\lambda)P(I|\lambda) \ (13.32)</script><p>它的参数学习可以由EM算法实现。</p>
<p><strong>1.确定完全数据的对数似然函数</strong></p>
<p>所有观测数据写成<script type="math/tex">O=(o_1,o_2,\cdots,o_T)</script>，所有隐数据写成<script type="math/tex">I=(i_1,i_2,\cdots,i_T)</script>，完全数据是<script type="math/tex">(O,I)=(o_1,o_2,\cdots,o_T,i_1,i_2,\cdots,i_T)</script>。完全数据的对数似然函数是<script type="math/tex">\log P(O,I|\lambda)</script>。</p>
<p><strong>2.EM算法的E步：求Q函数<script type="math/tex">Q(\lambda,\bar{\lambda})</script></strong></p>
<script type="math/tex; mode=display">
Q(\lambda,\bar{\lambda})=\sum_I \log P(O,I|\lambda)P(O,I|\bar{\lambda}) \ (13.33)</script><blockquote>
<p><strong>注意</strong>：根据Q函数的定义，式(13.33)把<script type="math/tex">P(I|\bar{\lambda})</script>替换为<script type="math/tex">\frac{P(O,I|\bar{\lambda})}{P(O|\bar{\lambda})}</script>，对<script type="math/tex">\lambda</script>而言，<script type="math/tex">\frac{1}{P(O|\bar{\lambda})}</script>是常数项，因此省略掉，从而得到式(13.33)。</p>
</blockquote>
<p>其中，<script type="math/tex">\bar{\lambda}</script>是隐马尔可夫模型参数的当前估计值，<script type="math/tex">\lambda</script>是要极大化的隐马尔可夫模型参数。</p>
<script type="math/tex; mode=display">
P(O,I|\lambda)= \pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)\cdots a_{i_{T-1}i_T}b_{i_T}(o_T)</script><p>于是函数<script type="math/tex">Q(\lambda,\bar{\lambda})</script>可以写成:</p>
<script type="math/tex; mode=display">
\begin{aligned}Q(\lambda,\bar{\lambda})&= \sum_I \log \pi_{i_1} P(O,I|\bar{\lambda})\\&+\sum_I \left(\sum_{t=1}^{T-1} \log a_{i_ti_{t+1}}\right) P(O,I|\bar{\lambda})+\sum_I \left(\sum_{t=1}^T \log b_{i_t}(o_t)\right)P(O,I|\bar{\lambda})\end{aligned} \ (13.34)</script><p>式中，求和都是对<strong>所有训练数据</strong>的序列总长度T进行的。</p>
<p><strong>3.EM算法的M步:极大化Q函数<script type="math/tex">Q(\lambda,\bar{\lambda})</script>求模型参数A,B,<script type="math/tex">\pi</script></strong></p>
<p>由于要极大化的参数在式（13.34）中单独地出现在3个项中，所以只需对各项分别极大化。</p>
<p>(1)式(13.34)的第一项可以写成:</p>
<script type="math/tex; mode=display">
\sum_I \log \pi_{i_0}P(O,I|\bar{\lambda})=\sum_{i=1}^N \log \pi_i P(O,i_1=i|\bar{\lambda})</script><p>注意到<script type="math/tex">\pi_i</script>满足约束条件<script type="math/tex">\sum_{i=1}^N \pi_i = 1</script>，利用拉格朗日乘子法，写出拉格朗日函数:</p>
<script type="math/tex; mode=display">
\sum_{i=1}^N \log \pi_i P(O,i_1=i|\bar{\lambda})+\gamma\left(\sum_{i=1}^N \pi_i -1\right)</script><p>对其求偏导数并令结果为0</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \pi_i} \left[\sum_{i=1}^N \log \pi_i P(O,i_1=i|\bar{\lambda})+\gamma\left(\sum_{i=1}^N \pi_i-1\right)\right]=0</script><p>得</p>
<script type="math/tex; mode=display">
P(O,i_1=i|\bar{\lambda})+\gamma \pi_i = 0</script><p>对i求和得到<script type="math/tex">\gamma</script></p>
<script type="math/tex; mode=display">
\gamma = - P(O|\bar{\lambda})</script><p>代入式(13.35)即得</p>
<script type="math/tex; mode=display">
\pi_i = \frac{P(O,i_1=1|\bar{\lambda})}{P(O|\bar{\lambda})} \ (13.36)</script><p>(2)式（13.34）的第2项可以写成</p>
<script type="math/tex; mode=display">
\sum_I \left(\sum_{t=1}^{T-1} \log a_{i_t,i_{t+1}}\right)P(O,I|\bar{\lambda})=\sum_{i=1}^N\sum_{j=1}^N\sum_{t=1}^{T-1} \log a_{ij}P(O,i_t=i,i_{t+1}=j|\bar{\lambda})</script><p>类似第1项，应用具有约束条件<script type="math/tex">\sum_{j=1}^N a_{ij}=1</script>的拉格朗日乘子法可以求出</p>
<script type="math/tex; mode=display">
a_{ij} = \frac{\sum_{t=1}^{T-1}P(O,i_t=i,i_{t+1}=j|\bar{\lambda})}{\sum_{t=1}^{T-1}P(O,i_t=i|\bar{\lambda})} \ (13.37)</script><p>(3)式(13.34)的第3项为</p>
<script type="math/tex; mode=display">
\sum_I \left(\sum_{t=1}^T \log b_{i_t}(o_t)\right)P(O,I|\bar{\lambda})=\sum_{j=1}^N \sum_{t=1}^T \log b_j(o_t)P(O,i_t=j|\bar{\lambda})</script><p>同样用拉格朗日乘子法，约束条件是<script type="math/tex">\sum_{k=1}^M b_j(k)=1</script>(参考公式(13.31))。注意，只有在<script type="math/tex">o_t=v_k</script>时<script type="math/tex">b_j(o_t)</script>对<script type="math/tex">b_j(k)</script>的偏导数才不为0，以<script type="math/tex">I(o_t=v_k)</script>表示。求得</p>
<script type="math/tex; mode=display">
b_j(k) = \frac{\sum_{t=1}^T P(O,i_t=j|\bar{\lambda})I(o_t=v_k)}{\sum_{t=1}^T P(O,i_t=j|\bar{\lambda})} \ (13.38)</script><h3 id="13-3-3-Baum-Welch模型参数估计公式"><a href="#13-3-3-Baum-Welch模型参数估计公式" class="headerlink" title="13.3.3 Baum-Welch模型参数估计公式"></a>13.3.3 Baum-Welch模型参数估计公式</h3><p>将式(13.36)~式(13.38)中的各概率分别用<script type="math/tex">\gamma_t(i)</script>，<script type="math/tex">\xi_t(i,j)</script>表示，则可将相应的公式写成 :</p>
<script type="math/tex; mode=display">
a_{ij} = \frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\sum_{t=1}^{T-1}\gamma_t(i)} \ (13.39)</script><script type="math/tex; mode=display">
b_j(k)=\frac{\sum^{T}_{t=1,o_t=v_k}\gamma_t(j)}{\sum_{t=1}^T \gamma_t(j)} \ (13.40)</script><script type="math/tex; mode=display">
\pi_i = \gamma_1(i) \ (13.41)</script><p>其中，<script type="math/tex">\gamma_t(i)</script>，<script type="math/tex">\xi_t(i,j)</script>分别由式（13.24）及式(13.26)给出。式(13.39)~(13.41)就是Baum-Welch算法，它是EM算法在隐马尔科夫模型学习中的具体体现。</p>
<p><strong>算法13.4(Baum-Welch算法)</strong></p>
<p>输入：观测数据<script type="math/tex">O=(o_1,o_2,\cdots,o_T)</script>；</p>
<p>输出：隐马尔科夫模型参数</p>
<p>(1)初始化</p>
<p>对n=0，选取<script type="math/tex">a_{ij}^{(0)}</script>，<script type="math/tex">b_j^{(0)}(k)</script>，<script type="math/tex">\pi_i^{(0)}</script>，得到模型<script type="math/tex">\lambda^{(0)}=(A^{(0)},B^{(0)},\pi^{(0)})</script>。</p>
<p>(2)递推。对n=1,2,…,</p>
<script type="math/tex; mode=display">
a_{ij}^{(n+1)} = \frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\sum_{t=1}^{T-1}\gamma_t(i)}</script><script type="math/tex; mode=display">
b_j(k)^{(n+1)}=\frac{\sum^{T}_{t=1,o_t=v_k}\gamma_t(j)}{\sum_{t=1}^T \gamma_t(j)}</script><script type="math/tex; mode=display">
\pi_i^{(n+1)} = \gamma_1(i)</script><p>右端各值按规则<script type="math/tex">O=(o_1,o_2,\cdots,o_T)</script>和模型<script type="math/tex">\lambda^{(n)}=(A^{(n)},B^{(n)},\pi^{(n)})</script>计算。式中<script type="math/tex">\gamma_t(i)</script>，<script type="math/tex">\xi_t(i,j)</script>由式(13.24)和式(13.26)给出。</p>
<p>(3)终止。得到模型参数<script type="math/tex">\lambda^{(n+1)}=(A^{(n+1)},B^{(n+1)},\pi^{(n+1)})</script>。</p>
<h2 id="13-4-预测算法"><a href="#13-4-预测算法" class="headerlink" title="13.4 预测算法"></a>13.4 预测算法</h2><p>下面介绍隐马尔科夫模型预测的两种算法:近似算法与维特比算法。</p>
<h3 id="13-4-1-近似算法"><a href="#13-4-1-近似算法" class="headerlink" title="13.4.1 近似算法"></a>13.4.1 近似算法</h3><p>近似算法的想法是，<strong>在每个时刻t选择在该时刻最有可能出现的状态<script type="math/tex">i_t^*</script>，从而得到一个状态序列<script type="math/tex">I^*=(i_1^*,i_2^*,\cdots,i_T^*)</script>，将它作为预测的结果</strong>。</p>
<p>给定隐马尔科夫模型<script type="math/tex">\lambda</script>和观测序列O，在时刻t处于状态<script type="math/tex">q_i</script>的概率<script type="math/tex">\gamma_t(i)</script>是</p>
<script type="math/tex; mode=display">
\gamma_t(i) = \frac{\alpha_t(i)\beta_t(i)}{P(O|\lambda)} = \frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N \alpha_t(j)\beta_t(j)} \ (13.42)</script><p>在每一时刻t最有可能的状态<script type="math/tex">i_t^*</script>是</p>
<script type="math/tex; mode=display">
i_t^* = \arg \max_{1\le i \le N} [\gamma_t(i)] , t=1,2,\cdots,T \ (13.43)</script><p>从而得到状态序列<script type="math/tex">I^*=(i_1^*,i_2^*,\cdots,i_T^*)</script>。</p>
<p><strong>近似算法的优点</strong>：计算简单</p>
<p><strong>近似算法的缺点：</strong>不能保证预测的状态序列整体是最有可能的状态序列，因为预测的状态序列可能有实际不发生的部分。(事实上，上述方法得到的状态序列中有可能存在转移概率为0的相邻状态，<strong>即对某些i,j，<script type="math/tex">a_{ij}=0</script></strong>)</p>
<h3 id="13-4-2-维特比算法"><a href="#13-4-2-维特比算法" class="headerlink" title="13.4.2 维特比算法"></a>13.4.2 维特比算法</h3><p>维特比算法的思路是，<strong>使用动态规划解隐马尔科夫模型预测问题，即用动态规划求概率最大路径(最优路径)</strong>。(这一条路径对应着一个状态序列)。</p>
<p><strong>根据动态规划原理</strong>，最优路径具有这样的特性：如果最优路径在时刻t通过结点<script type="math/tex">i_t^*</script>，那么这一路径从结点<script type="math/tex">i_t^*</script>到终点<script type="math/tex">i_T^*</script>的部分路径，对于从<script type="math/tex">i_t^*</script>到<script type="math/tex">i_T^*</script>的所有可能的部分路径来说，必须是最优的。</p>
<p><strong>以下描述维特比算法的流程：</strong></p>
<p>依据这一原理，我们只需从时刻t=1开始，递推地计算在时刻t状态为i的各条部分路径的最大概率，直至得到时刻t=T状态为i的各条路径的最大概率。</p>
<p>时刻t=T的最大概率即为最优路径的概率<script type="math/tex">P^*</script>，最优路径的终结点<script type="math/tex">i_T^*</script>也同时得到。</p>
<p>之后，为了找出最优路径的各个结点，从终结点<script type="math/tex">i^*_T</script>开始，由后向前逐步求得结点<script type="math/tex">i^*_{T-1},\cdots,i_1^*</script>，得到最优路径<script type="math/tex">I^*=(i_1^*,i_2^*,\cdots,i_T^*)</script>。<strong>这就是维特比算法</strong>。</p>
<p>首先导入两个变量<script type="math/tex">\delta</script>和<script type="math/tex">\Psi</script>。定义在时刻t状态为i的所有单个路径<script type="math/tex">(i_1,i_2,\cdots,i_t)</script>中概率最大值为：</p>
<script type="math/tex; mode=display">
\delta_t(i) = \max_{i_1,i_2,\cdots,i_{t-1}}P(i_t=i,i_{t-1},\cdots,i_1,o_t,\cdots,o_1|\lambda),i=1,2,\cdots,N \ (13.44)</script><p>由定义可得变量<script type="math/tex">\delta</script>的递推公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}\delta_{t+1}(i) &= \max_{i_1,i_2,\cdots,i_t}P(i_{t+1}=i,i_t,\cdots,i_1,o_{t+1},\cdots,o_1|\lambda)\\&=\max_{1\le j\le N} [\delta_t(j)\alpha_{ji}]b_i(o_{t+1}), i=1,2,\cdots,N;t=1,2,\cdots,T-1\end{aligned} \ (13.45)</script><p>定义在时刻t状态为i的所有单个路径<script type="math/tex">(i_1,i_2,\cdots,i_{t-1},i)</script>中概率最大的路径的第t-1个结点为：</p>
<script type="math/tex; mode=display">
\Psi_t(i) = \arg \max_{1\le j \le N}[\delta_{t-1}(j)a_{ij}],i=1,2,\cdots,N \ (13.46)</script><p><strong>算法13.5(维特比算法)</strong></p>
<p>输入：模型<script type="math/tex">\lambda=(A,B,\pi)</script>和观测<script type="math/tex">O=(o_1,o_2,\cdots,o_T)</script>；</p>
<p>输出：最优路径<script type="math/tex">I^*=(i_1^*,i_2^*,\cdots,i_T^*)</script></p>
<p>(1)初始化</p>
<script type="math/tex; mode=display">
\delta_1(i) = \pi_i b_i(o_1), i=1,2,\cdots,N</script><script type="math/tex; mode=display">
\Psi_1(i) = 0,i=1,2,\cdots,N</script><p>(2)递推。对t=2,3,…,T</p>
<script type="math/tex; mode=display">
\delta_t(i) = \max_{1\le j\le N}[\delta_{t-1}(j)a_{ij}]b_i(o_t) , i=1,2,\cdots,N</script><script type="math/tex; mode=display">
\Psi_t(i) = \arg\max_{i\le j \le N}[\delta_{t-1}(j)a_{ij}],i=1,2,\cdots,N</script><p>(3)终止</p>
<script type="math/tex; mode=display">
P^* = \max_{1\le i \le N}\delta_T(i)</script><script type="math/tex; mode=display">
i^*_T = \arg\max_{1\le i\le N}[\delta_T(i)]</script><p>(4)最优路径回溯。对t=T-1,T-2,…,1</p>
<script type="math/tex; mode=display">
i_t^* = \Psi_{t+1}(i_{t+1}^*)</script><p>求得最优路径<script type="math/tex">I^*=(i_1^*,i_2^*,\cdots,i_T^*)</script>。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/生成模型/" rel="tag"># 生成模型</a>
          
            <a href="/tags/标注模型/" rel="tag"># 标注模型</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/06/14/EM算法及其推广/" rel="next" title="EM算法及其推广">
                <i class="fa fa-chevron-left"></i> EM算法及其推广
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/06/14/条件随机场/" rel="prev" title="条件随机场">
                条件随机场 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">jozee</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#13-1-隐马尔科夫模型的基本概念"><span class="nav-number">1.</span> <span class="nav-text">13.1 隐马尔科夫模型的基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#13-1-1隐马尔科夫模型的定义"><span class="nav-number">1.1.</span> <span class="nav-text">13.1.1隐马尔科夫模型的定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-1-2-观测序列的生成过程"><span class="nav-number">1.2.</span> <span class="nav-text">13.1.2 观测序列的生成过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-1-3-隐马尔科夫模型的3个基本问题"><span class="nav-number">1.3.</span> <span class="nav-text">13.1.3 隐马尔科夫模型的3个基本问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#13-2-概率计算算法"><span class="nav-number">2.</span> <span class="nav-text">13.2 概率计算算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#13-2-1-直接计算法"><span class="nav-number">2.1.</span> <span class="nav-text">13.2.1 直接计算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-2-2-前向算法"><span class="nav-number">2.2.</span> <span class="nav-text">13.2.2 前向算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-2-3-后向算法"><span class="nav-number">2.3.</span> <span class="nav-text">13.2.3 后向算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-2-4-一些概率与期望值的计算"><span class="nav-number">2.4.</span> <span class="nav-text">13.2.4 一些概率与期望值的计算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#13-3-学习算法"><span class="nav-number">3.</span> <span class="nav-text">13.3 学习算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#13-3-1-监督学习算法"><span class="nav-number">3.1.</span> <span class="nav-text">13.3.1 监督学习算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-3-2-Baum-Welch算法"><span class="nav-number">3.2.</span> <span class="nav-text">13.3.2 Baum-Welch算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-3-3-Baum-Welch模型参数估计公式"><span class="nav-number">3.3.</span> <span class="nav-text">13.3.3 Baum-Welch模型参数估计公式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#13-4-预测算法"><span class="nav-number">4.</span> <span class="nav-text">13.4 预测算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#13-4-1-近似算法"><span class="nav-number">4.1.</span> <span class="nav-text">13.4.1 近似算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-4-2-维特比算法"><span class="nav-number">4.2.</span> <span class="nav-text">13.4.2 维特比算法</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">jozee</span>

  
</div>








<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv"></span>次
</span>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("", "");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
