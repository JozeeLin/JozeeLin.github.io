<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="XGBoost,Ensemble,Gradient Boosting,">










<meta name="description" content="论文：XGBoost: A Scalable Tree Boosting System 1.1 决策树决策树学习本质上是从训练数据集中归纳出一组分类规则。 学习过程通常包含3个步骤：  特征选择(评估标准：信息增益，信息增益比，基尼指数，平方误差) 决策树的生成算法(ID3,C4.5,CART) 决策树的修剪(前剪枝early stop,后剪枝相当于regularization)  这里使用ID3">
<meta name="keywords" content="XGBoost,Ensemble,Gradient Boosting">
<meta property="og:type" content="article">
<meta property="og:title" content="XGBoost论文解读">
<meta property="og:url" content="http://yoursite.com/2019/07/19/XGBoost/index.html">
<meta property="og:site_name" content="Jozee&#39;s技术博客">
<meta property="og:description" content="论文：XGBoost: A Scalable Tree Boosting System 1.1 决策树决策树学习本质上是从训练数据集中归纳出一组分类规则。 学习过程通常包含3个步骤：  特征选择(评估标准：信息增益，信息增益比，基尼指数，平方误差) 决策树的生成算法(ID3,C4.5,CART) 决策树的修剪(前剪枝early stop,后剪枝相当于regularization)  这里使用ID3">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/dt-1-data.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/dt-id3.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/xgboost-qx.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/xgboost-predict.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/xgboost-complexity.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/xgboost-gain.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/xgboost-exact.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/xgboost-split.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/xgboost-approximate.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/xgboost-quantile-split.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/xgboost-approximate-split-2.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/xgboost-split-1.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/xgboost-split-quantile.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/xgboost-sparsity.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/xgboost-block.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/xgboost-cache-1.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/xgboost-cache-2.png">
<meta property="og:image" content="http://yoursite.com/2019/07/19/XGBoost/image/xgboost-cache-3.png">
<meta property="og:updated_time" content="2019-07-19T06:27:04.730Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="XGBoost论文解读">
<meta name="twitter:description" content="论文：XGBoost: A Scalable Tree Boosting System 1.1 决策树决策树学习本质上是从训练数据集中归纳出一组分类规则。 学习过程通常包含3个步骤：  特征选择(评估标准：信息增益，信息增益比，基尼指数，平方误差) 决策树的生成算法(ID3,C4.5,CART) 决策树的修剪(前剪枝early stop,后剪枝相当于regularization)  这里使用ID3">
<meta name="twitter:image" content="http://yoursite.com/2019/07/19/XGBoost/image/dt-1-data.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/07/19/XGBoost/">





  <title>XGBoost论文解读 | Jozee's技术博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jozee's技术博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/19/XGBoost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jozee">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jozee's技术博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">XGBoost论文解读</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-19T13:51:58+08:00">
                2019-07-19
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/07/19/XGBoost/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/07/19/XGBoost/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          <span id="busuanzi_container_page_pv">
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon">
            <i class="fa fa-eye"></i>
            </span>
            阅读量: <span id="busuanzi_value_page_pv"></span>次
          </span>

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>论文：<a href="https://arxiv.org/pdf/1603.02754.pdf" target="_blank" rel="noopener">XGBoost: A Scalable Tree Boosting System</a></p>
<h2 id="1-1-决策树"><a href="#1-1-决策树" class="headerlink" title="1.1 决策树"></a>1.1 决策树</h2><p>决策树学习<strong>本质上是从训练数据集中归纳出一组分类规则</strong>。</p>
<p>学习过程通常包含3个步骤：</p>
<ol>
<li>特征选择(评估标准：信息增益，信息增益比，基尼指数，平方误差)</li>
<li>决策树的生成算法(ID3,C4.5,CART)</li>
<li>决策树的修剪(前剪枝early stop,后剪枝相当于regularization)</li>
</ol>
<p>这里使用ID3算法来简单说明决策树的生成过程。ID3算法中使用信息增益作为特征选择的评估标准，更过关于信息增益的知识请参考<a href="https://jozeelin.github.io/2019/06/14/%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">统计学习方法-信息增益</a></p>
<ul>
<li><p>给定数据集：</p>
<p><img src="image/dt-1-data.png" alt="dt-1-data"></p>
</li>
<li><p>ID3算法的流程：</p>
<p>输入：训练数据集D，特征集A，阈值<script type="math/tex">\varepsilon</script></p>
<p>输出：决策树T</p>
<ol>
<li>若D中所有实例属于同一类<script type="math/tex">C_k</script>，则T为单结点树，并将类<script type="math/tex">C_k</script>作为该结点的类标记，返回T。</li>
<li>若<script type="math/tex">A=\varnothing</script>，则T为单节点树，并将D中实例数最大的类<script type="math/tex">C_k</script>作为该节点的类标记，返回T。</li>
<li>否则，按信息增益算法计算A中各特征对D的信息增益，选择信息增益最大的特征<script type="math/tex">A_g</script>。</li>
<li>如果<script type="math/tex">A_g</script>的信息增益小于阈值<script type="math/tex">\varepsilon</script>，则置T为单节点树，并将D中实例数最大的类<script type="math/tex">C_k</script>作为该节点的类标记，返回T</li>
<li>否则，对<script type="math/tex">A_g</script>的每一可能值<script type="math/tex">a_i</script>，依<script type="math/tex">A_g=a_i</script>将D分割为若干非空子集<script type="math/tex">D_i</script>，将<script type="math/tex">D_i</script>中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树T，返回T</li>
<li>对第i个子节点，以<script type="math/tex">D_i</script>为训练集，以A-<script type="math/tex">\{A_g\}</script>为特征集，递归地调用步(1)~步(5)，得到子树<script type="math/tex">T_i</script>，返回<script type="math/tex">T_i</script>。</li>
</ol>
</li>
</ul>
<p>执行ID3算法中的步骤3：根据信息增益的定义，遍历所有的特征，得到特征“有自己的房子”的信息增益值最大，所以选择特征”有自己的房子”作为根结点的特征。它将训练数据集D划分为两个子集D1(有自己的房子-是)，和D2(有自己的房子-否)。</p>
<p>由于D1只有同一类的样本点，满足终止条件(ID3算法中的步骤1)，所以作为一个叶子结点(类标记)。</p>
<p>D2则需要从剩下的特征：年龄、有工作、信贷情况中选择新的特征，执行ID3算法中的步骤3，得到信息增益最大的特征是”有工作”，因此选择它作为新的结点，它将训练数据集D2划分为D21,D22，由于划分得到的两个子集都满足终止条件(ID3算法中的步骤1)，因此构建决策树结束。</p>
<blockquote>
<p>　注意：这里只考虑了算法ID3 中其中两个个终止条件(即步骤1和步骤2，由于这里只使用了2个特征就可以把所有样本正确分开，所以没有出发步骤2中的终止条件)，还有一个终止条件，即步骤4，这里我们并没有给出一个阈值，同时判断当前信息增益与阈值的比较情况。</p>
</blockquote>
<p>通过以上的决策树构建过程，最终得到的决策树如下所示：</p>
<p><img src="image/dt-id3.png" alt="dt-id3"></p>
<h2 id="2-1-CART树"><a href="#2-1-CART树" class="headerlink" title="2.1 CART树"></a>2.1 CART树</h2><p>关于CART树更多的细节及python实现请参考<a href="https://jozeelin.github.io/2019/06/14/%E5%86%B3%E7%AD%96%E6%A0%91/" target="_blank" rel="noopener">统计学习方法-回归树生成</a></p>
<p>这里只讲解CART回归树。</p>
<p>假设X与Y分别为输入和输出变量，并且Y是连续变量，给定训练数据集:</p>
<script type="math/tex; mode=display">
D = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}</script><p>考虑如何生成回归树。</p>
<p>假设已将输入空间划分为M个单元<script type="math/tex">R_1,R_2,...,R_M</script>，并且在每个单元<script type="math/tex">R_m</script>上有一个固定的输出值<script type="math/tex">c_m</script>，于是回归树模型可表示为:</p>
<script type="math/tex; mode=display">
f(x) = \sum_{m=1}^M c_m I(x \in R_m) \tag{2-1}</script><p>当输入空间的划分确定时，可以用平方误差<script type="math/tex">\sum_{x_i\in R_m}(y_i - f(x_i))^2</script>来表示回归树对于训练数据的预测误差。</p>
<p><strong>用平方误差最小的准则求解每个单元上的最优输出值</strong>。易知，单元<script type="math/tex">R_m</script>上的<script type="math/tex">c_m</script>的最优值<script type="math/tex">\hat{c}_m</script>是<script type="math/tex">R_m</script>上的所有输入实例<script type="math/tex">x_i</script>对应的输出<script type="math/tex">y_i</script>的均值，即</p>
<script type="math/tex; mode=display">
\hat{c}_m = \mathrm{ave}(y_i|x_i \in R_m) \tag{2-2}</script><p>这里采用<strong>启发式方法</strong>对输入空间进行划分。选择第j个变量<script type="math/tex">x^{(j)}</script>和它取的值s，作为切分变量和切分点并定义两个区域:</p>
<script type="math/tex; mode=display">
R_1(j,s) = \{x|x^{(j)} \le s\} \ , R_2(j,s) = \{x|x^{(j)} > s\} \tag{2-3}</script><p>然后寻找最优切分变量j和最优切分点s。具体地，求解:</p>
<script type="math/tex; mode=display">
\min_{j,s} \left[ \min_{c_1} \sum_{x_i \in R_1(j,s)} (y_i-c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i-c_2)^2 \right] \tag{2-4}</script><p>对固定输入变量j可以找到最优切分点s。</p>
<script type="math/tex; mode=display">
\hat{c}_1 = \mathrm{ave}(y_i|x_i \in R_1(j,s)) \ , \hat{c}_2 = \mathrm{ave}(y_i|x_i \in R_2(j,s))</script><p><strong>遍历所有输入变量</strong>，找到最优的切分变量j，构成一个对(j,s)。以此将输入空间划分为两个区域。</p>
<p>接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成一颗回归树。<strong>通常称为最小二乘回归树</strong>，现将算法叙述如下:</p>
<p><strong>算法(最小二乘回归树生成算法)</strong></p>
<p>输入:训练数据集D</p>
<p>输出:回归树f(x)</p>
<p>在训练数据集所在的输入空间中，<strong>递归的将每个区域划分为两个子区域</strong>并<strong>决定每个子区域上的输出值</strong>，构建二叉决策树：</p>
<ol>
<li><p>选择最优切分变量j与切分点s，求解</p>
<script type="math/tex; mode=display">
\min_{j,s} \left[ \min_{c_1} \sum_{x_i \in R_1(j,s)} (y_i-c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i-c_2)^2 \right] \tag{2-5}</script><p>遍历变量j，对固定的切分变量j扫描切分点s，选择使式(2-5)达到最小值的对(j,s)。</p>
</li>
<li><p>用选定的对(j,s)划分区域并决定相应的输出值:</p>
<script type="math/tex; mode=display">
R_1(j,s) = \{x|x^{(j)} \le s\} \ , R_2(j,s)=\{x|x^{(j)} > s\}</script><script type="math/tex; mode=display">
\hat{c}_m = \frac{1}{N_m} \sum_{x_i \in R_m(j,s)} y_i \ , x \in R_m \ , m=1,2</script></li>
<li><p>继续对两个子区域调用步骤(1)(2)，直至满足停止条件</p>
</li>
<li><p>将输入空间划分为M个区域<script type="math/tex">R_1,R_2,...,R_M</script>，生成决策树:</p>
<script type="math/tex; mode=display">
f(x) = \sum_{m=1}^M c_m I(x \in R_m)</script></li>
</ol>
<h2 id="3-1-AdaBoost与前向分步算法"><a href="#3-1-AdaBoost与前向分步算法" class="headerlink" title="3.1 AdaBoost与前向分步算法"></a>3.1 AdaBoost与前向分步算法</h2><h3 id="3-1-1-AdaBoost算法"><a href="#3-1-1-AdaBoost算法" class="headerlink" title="3.1.1 AdaBoost算法"></a>3.1.1 AdaBoost算法</h3><p>假设给定一个二类分类的训练数据集<script type="math/tex">T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}</script>，其中，每个样本点由实例与标记组成。实例<script type="math/tex">x_i \in \mathcal{X} \subseteq R^n</script>，标记<script type="math/tex">y_i \in \mathcal{Y} = \{-1,+1\}</script>，<script type="math/tex">\mathcal{X}</script>是实例空间，<script type="math/tex">\mathcal{Y}</script>是标记集合。</p>
<p>AdaBoost利用以下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器线性组合成为一个强分类器。</p>
<p><strong>算法(AdaBoost)</strong></p>
<p>输入：训练数据集<script type="math/tex">T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}</script>，其中<script type="math/tex">x_i \in \mathcal{X} \subseteq R^n</script>，<script type="math/tex">y_i \in \mathcal{Y}=\{-1,+1\}</script>；弱学习算法；</p>
<p>输出：最终分类器<script type="math/tex">G(x)</script></p>
<p>(1)初始化训练数据的权值分布</p>
<script type="math/tex; mode=display">
D_1 = (w_{11},\cdots,w_{1i},\cdots,w_{1N}) , w_{1i} = \frac{1}{N},i=1,2,\cdots,N</script><p>(2)对m=1,2,…,M(表示有 M个弱分类器)</p>
<p>​    (a)使用具有权值分布<script type="math/tex">D_m</script>的训练数据集学习，得到基本分类器:</p>
<script type="math/tex; mode=display">
G_m(x):\mathcal{X} \rightarrow \{-1,+1\}</script><p>​    (b)计算<script type="math/tex">G_m(x)</script>在训练数据集上的分类误差率</p>
<script type="math/tex; mode=display">
e_m = \sum_{i=1}^N P(G_m(x_i)\ne y_i) = \sum_{i=1}^N w_{mi}I(G_m(x_i)\ne y_i) \tag{3-1}</script><p>​    (c)计算<script type="math/tex">G_m(x)</script>的系数</p>
<script type="math/tex; mode=display">
\alpha_m = \frac{1}{2} \log \frac{1-e_m}{e_m} \tag{3-2}</script><p>​    这里的对数是自然对数。</p>
<p>​    (d)更新训练数据集的权值分布</p>
<script type="math/tex; mode=display">
D_{m+1} = (w_{m+1,1},\cdots,w_{m+1,i},\cdots,w_{m+1,N}) \tag{3-3}</script><script type="math/tex; mode=display">
w_{m+1,i} = \frac{w_{mi}}{Z_m}\exp (-\alpha_m y_i G_m(x_i)) \ , i=1,2,\cdots,N \tag{3-4}</script><p>​    这里，<script type="math/tex">Z_m</script>是规范化因子</p>
<script type="math/tex; mode=display">
Z_m = \sum_{i=1}^N w_{mi}\exp(-\alpha_m y_i G_m(x_i)) \tag{3-5}</script><p>(3)构建基本分类器的线性组合</p>
<script type="math/tex; mode=display">
f(x) = \sum_{m=1}^M \alpha_m G_m(x) \tag{3-6}</script><p>得到最终分类器</p>
<script type="math/tex; mode=display">
G(x) = \mathrm{sign}(f(x)) = \mathrm{sign} \left(\sum_{m=1}^M \alpha_m G_m(x)\right) \tag{3-7}</script><p><strong>对AdaBoost算法作如下说明:</strong></p>
<p><strong><u>步骤(1)</u></strong> <strong>假设训练数据集具有均匀的权值分布(即每个训练样本在基本分类器的学习中作用相同)</strong>；在此权值分布下，基于原始数据训练出基本分类器<script type="math/tex">G_1(x)</script></p>
<p><strong><u>步骤(2)</u></strong>AdaBoost反复学习基本分类器，在每一轮m=1,2,…,M顺次的执行下列操作:</p>
<p>​    <strong>(a)</strong>使用当前分布<script type="math/tex">D_m</script>加权的训练数据集，学习基本分类器<script type="math/tex">G_m(x)</script>。</p>
<p>​    <strong>(b)</strong>计算基本分类器<script type="math/tex">G_m(x)</script>在加权训练数据集上的分类误差率:</p>
<script type="math/tex; mode=display">
e_m = \sum_{i=1}^N P(G_m(x_i) \ne y_i) = \sum_{G_m(x_i) \ne y_i} w_{mi} \tag{3-8}</script><p>​    这里，<script type="math/tex">w_{mi}</script>表示第m轮第i个样本点的权值，<script type="math/tex">\sum_{i=1}^N w_{mi}=1</script>。这表明，<strong><script type="math/tex">G_m(x)</script>在加权的训练数据集上的分类误差率是被<script type="math/tex">G_m(x)</script>误分类的样本的权值之和</strong>。</p>
<p>​    <strong>(c)</strong>计算基本分类器<script type="math/tex">G_m(x)</script>的系数<script type="math/tex">\alpha_m</script>，<script type="math/tex">\alpha_m</script>表示<script type="math/tex">G_m(x)</script>在最终分类器中的重要性。由式(3-2)可知，当<script type="math/tex">e_m \le \frac{1}{2}</script>时，<script type="math/tex">\alpha_m \ge 0</script>，并且<script type="math/tex">\alpha_m</script>随着<script type="math/tex">e_m</script>的减小而增大。<strong>相当于误分类率越小的基本分类器在最终分类器中的重要性越大</strong>。</p>
<p>​    <strong>(d)</strong>更新训练数据的权值分布。式(3-4)可以写成:</p>
<script type="math/tex; mode=display">
w_{m+1,i} = \left\{\begin{aligned}\frac{w_{mi}}{Z_m}e^{-\alpha_m},&G_m(x_i)=y_i \\ \frac{w_{mi}}{Z_m}e^{\alpha_m},&G_m(x_i)\ne y_i\end{aligned}\right.</script><p>​    由此可知，<strong>被基本分类器<script type="math/tex">G_m(x)</script>误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小</strong>。</p>
<p>​    由式(3-2)可知误分类样本的权值被放大<script type="math/tex">e^{2\alpha_m} = \frac{1-e_m}{e_m}</script>倍。</p>
<p><strong><u>步骤(3)</u></strong> 线性组合<script type="math/tex">f(x)</script>实现M个基本分类器的加权表决。系数<script type="math/tex">\alpha_m</script>表示了基本分类器<script type="math/tex">G_m(x)</script>重要性，这里，<strong>所有<script type="math/tex">\alpha_m</script>之和并不为1</strong>。<script type="math/tex">f(x)</script>的<strong>符号决定实例x的类</strong>，<strong><script type="math/tex">f(x)</script>的绝对值表示分类的确信度</strong>。</p>
<blockquote>
<p>关于adaboost的更详细的说明及python实现请参考<a href="[https://jozeelin.github.io/2019/06/14/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/](https://jozeelin.github.io/2019/06/14/提升方法/">统计学习方法-adaboost</a>)</p>
</blockquote>
<h3 id="3-1-2-前向分步算法"><a href="#3-1-2-前向分步算法" class="headerlink" title="3.1.2 前向分步算法"></a>3.1.2 前向分步算法</h3><p>考虑加法模型:</p>
<script type="math/tex; mode=display">
f(x) = \sum_{m=1}^M \beta_m b(x;\gamma_m) \tag{3-9}</script><p>其中，<script type="math/tex">b(x;\gamma_m)</script>为基函数，<script type="math/tex">\gamma_m</script>为基函数的参数，<script type="math/tex">\beta_m</script>为基函数的系数。由此可见，式(3-6)是加法模型。</p>
<p>在给定训练数据及损失函数<script type="math/tex">L(y,f(x))</script>的条件下，学习加法模型<script type="math/tex">f(x)</script>称为经验风险极小化即损失函数极小化问题:</p>
<script type="math/tex; mode=display">
\min_{\beta_m,\gamma_m} \sum_{i=1}^N L\left(y_i,\sum_{m=1}^M \beta_m b(x_i;\gamma_m)\right) \tag{3-10}</script><p><strong>前向分步算法的求解思路是</strong>：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式(3-10)，那么就可以简化优化的复杂度。具体地，每步只需优化如下损失函数:</p>
<script type="math/tex; mode=display">
\min_{\beta,\gamma} \sum_{i=1}^N L(y_i,\beta b(x_i;\gamma)) \tag{3-11}</script><p>给定训练数据集<script type="math/tex">T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}</script>，<script type="math/tex">x_i \in \mathcal{X} \subseteq R^n</script>，<script type="math/tex">y_i \in \mathcal{Y} = \{-1,+1\}</script>。损失函数<script type="math/tex">L(y,f(x))</script>和基函数的集合<script type="math/tex">\{b(x;\gamma)\}</script>，学习加法模型<script type="math/tex">f(x)</script>的前向分步算法如下:</p>
<p><strong>算法(前向分步算法)</strong></p>
<p>输入:训练数据集<script type="math/tex">T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}</script>；损失函数<script type="math/tex">L(y,f(x))</script>；基函数集<script type="math/tex">\{b(x;\gamma)\}</script>；</p>
<p>输出:加法模型<script type="math/tex">f(x)</script>。</p>
<p>(1)初始化<script type="math/tex">f_0(x)=0</script></p>
<p>(2)对m=1,2,…,M</p>
<p>​    (a)极小化损失函数</p>
<script type="math/tex; mode=display">
(\beta_m,\gamma_m)=\arg\min_{\beta,\gamma} \sum_{i=1}^N L(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma)) \tag{3-12}</script><p>​    得到参数<script type="math/tex">\beta_m</script>，<script type="math/tex">\gamma_m</script>。</p>
<p>​    (b)更新</p>
<script type="math/tex; mode=display">
f_m(x) = f_{m-1}(x)+\beta_mb(x;\gamma_m) \tag{3-13}</script><p>(3)得到加法模型</p>
<script type="math/tex; mode=display">
f(x) = f_M(x) = \sum_{m=1}^M \beta_mb(x;\gamma_m) \tag{3-14}</script><p>这样，<strong>前向分步算法将同时求解从m=1到M所有参数<script type="math/tex">\beta_m</script>，<script type="math/tex">\gamma_m</script>的优化问题简化为逐次求解各个<script type="math/tex">\beta_m</script>，<script type="math/tex">\gamma_m</script>的优化问题</strong>。</p>
<h3 id="3-1-3-AdaBoost算法与前向分步算法的关系"><a href="#3-1-3-AdaBoost算法与前向分步算法的关系" class="headerlink" title="3.1.3 AdaBoost算法与前向分步算法的关系"></a>3.1.3 AdaBoost算法与前向分步算法的关系</h3><p>AdaBoost算法还可以理解为:模型为<strong>加法模型</strong>、损失函数为<strong>指数函数</strong>、学习算法为<strong>前向分步算法</strong>时的学习方法。</p>
<p>关于AdaBoost算法与前向分步算法的等价性证明如下：</p>
<p>前向分步算法学习的是加法模型，<strong>当基函数为基本分类器时，该加法模型等价于AdaBoost的最终分类器</strong>：</p>
<script type="math/tex; mode=display">
f(x) = \sum_{m=1}^M \alpha_m G_m(x) \tag{3-15}</script><p>由基本分类器<script type="math/tex">G_m(x)</script>及其系数<script type="math/tex">\alpha_m</script>组成，m=1,2,…,M。前向分步算法逐一学习基函数，这一过程与AdaBoost算法逐一学习基本分类器的过程一致。</p>
<p><strong>接下来证明前向分步算法的损失函数是指数损失函数</strong>：</p>
<script type="math/tex; mode=display">
L(y,f(x)) = \exp[-yf(x)]</script><p><strong>时</strong>，其学习的具体操作等价于AdaBoost算法学习的具体操作。</p>
<p>假设经过m-1轮迭代前向分步算法已经得到<script type="math/tex">f_{m-1}(x)</script>:</p>
<script type="math/tex; mode=display">
\begin{aligned}f_{m-1}(x) &= f_{m-2}(x)+\alpha_{m-1}G_{m-1}(x)\\&=\alpha_1G_1(x)+\cdots+\alpha_{m-1}G_{m-1}(x)\end{aligned}</script><p>在第m轮迭代得到<script type="math/tex">\alpha_m</script>，<script type="math/tex">G_m(x)</script>和<script type="math/tex">f_m(x)</script>。</p>
<script type="math/tex; mode=display">
f_m(x) = f_{m-1}(x)+\alpha_mG_m(x)</script><p>目标是使前向分步算法得到的<script type="math/tex">\alpha_m</script>和<script type="math/tex">G_m(x)</script>使<script type="math/tex">f_m(x)</script>在训练数据集T上的指数损失最小，即</p>
<script type="math/tex; mode=display">
(\alpha_m,G_m(x)) = \arg\min_{\alpha,G} \sum_{i=1}^N\exp[-y_i(f_{m-1}(x_i)+\alpha G(x_i))] \tag{3-16}</script><p>式(3-16)可以表示为:</p>
<script type="math/tex; mode=display">
(\alpha_m,G_m(x)) = \arg\min_{\alpha,G} \sum_{i=1}^N \bar{w}_{mi}\exp[-y_i\alpha G(x_i)] \tag{3-17}</script><p>其中，<script type="math/tex">\bar{w}_{mi}=\exp[-y_if_{m-1}(x_i)]</script>。因为<script type="math/tex">\bar{w}_{mi}</script>既不依赖<script type="math/tex">\alpha</script>也不依赖于G，所以与最小化无关。但<script type="math/tex">\bar{w}_{mi}</script>依赖于<script type="math/tex">f_{m-1}(x)</script>，随着每一轮迭代而发生改变。</p>
<p><strong>证明式(3-17)的解<script type="math/tex">\alpha_m^*</script>和<script type="math/tex">G_m^*</script>就是AdaBoost算法所得到的<script type="math/tex">\alpha_m</script>和<script type="math/tex">G_m(x)</script></strong>。</p>
<p>求解式(3-17):</p>
<ol>
<li><p>求<script type="math/tex">G_m^*</script>。对任意<script type="math/tex">\alpha>0</script>，使式(3-17)最小的<script type="math/tex">G(x)</script>由下式得到：</p>
<script type="math/tex; mode=display">
G_m^* = \arg\min_G \sum_{i=1}^N \bar{w}_{mi}I(y_i \ne G(x_i))</script><p>其中，<script type="math/tex">\bar{w}_{mi} = \exp[-y_i f_{m-1}(x_i)]</script>。</p>
<p><strong>此分类器<script type="math/tex">G_m^*(x)</script>即为AdaBoost算法的基本分类器<script type="math/tex">G_m(x)</script>，因为它是使第m轮加权训练数据分类误差率最小的基本分类器</strong>。</p>
</li>
<li><p>求<script type="math/tex">\alpha_m^*</script>。式(3-17)可写成以下形式：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\sum_{i=1}^N \bar{w}_{mi} \exp[-y_i\alpha G(x_i)] &= \sum_{y_i=G_m(x_i)} \bar{w}_{mi}e^{-\alpha}+\sum_{y_i\ne G_m(x_i)} \bar{w}_{mi}e^{\alpha}\\ 
&=(e^{\alpha}-e^{-\alpha})\sum_{i=1}^N \bar{w}_{mi} I(y_i\ne G(x_i))+e^{-\alpha}\sum_{i=1}^N \bar{w}_{mi}

\end{aligned} \tag{3-18}</script><blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\sum_{y_i=G_m(x_i)} \bar{w}_{mi}e^{-\alpha} &= \bar{w}_{mi}e^{-\alpha}(\sum_{i=1}^N-\sum_{y_i\ne G_m(x_i)}) \\
&=\bar{w}_{mi}e^{-\alpha}(\sum_{i=1}^N 1-I(y_i\ne G_m(x_i)))
\end{aligned}</script></blockquote>
</li>
</ol>
<p>   将已求得的<script type="math/tex">G_m^*</script>代入式(3-18)，可得：</p>
<script type="math/tex; mode=display">
   \sum_{i=1}^N \bar{w}_{mi} \exp[-y_i\alpha G(x_i)] = (e^{\alpha}-e^{-\alpha})G_m^{*}+e^{-\alpha}\sum_{i=1}^N \bar{w}_{mi} \tag{3-19}</script><p>   对<script type="math/tex">\alpha</script>求导并使导数为0，即得到使式(3-17)最小的<script type="math/tex">\alpha</script>。</p>
<script type="math/tex; mode=display">
   \alpha_m^* = \frac{1}{2}\log \frac{1-e_m}{e_m}</script><p>   其中，<script type="math/tex">e_m</script>是分类误差率:</p>
<script type="math/tex; mode=display">
   e_m = \frac{\sum_{i=1}^N \bar{w}_{mi} I(y_i \ne G_m(x_i))}{\sum_{i=1}^N \bar{w}_{mi}}=\sum_{i=1}^N w_{mi}I(y_i \ne G_m(x_i))</script><p>   这里的<script type="math/tex">\alpha_m^*</script>与AdaBoost算法第2(c)步的<script type="math/tex">\alpha_m</script>完全一致。</p>
<p>   最后来看每一轮样本权值的更新。由</p>
<script type="math/tex; mode=display">
   f_m(x) = f_{m-1}(x)+\alpha_mG_m(x)</script><p>   以及<script type="math/tex">\bar{w}_{mi}=\exp[-y_i f_{m-1}(x_i)]</script>，可得</p>
<script type="math/tex; mode=display">
   \bar{w}_{m+1,i}=\bar{w}_{m,i} \exp[-y_i \alpha_m G_m(x)]</script><p>   这与AdaBoost算法第2(d)步的样本权值的更新，只相差规范化因子，因而等价。</p>
<h2 id="4-1-回归问题提升树的前向分布算法"><a href="#4-1-回归问题提升树的前向分布算法" class="headerlink" title="4.1 回归问题提升树的前向分布算法"></a>4.1 回归问题提升树的前向分布算法</h2><p>对应着二分类问题的adaboost算法，回归问题提升树对应的加法模型如下所示：</p>
<p>已知一个训练数据集<script type="math/tex">T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\},x_i \in \mathcal{X} \subseteq R^n,\mathcal{X}</script>为输入空间，<script type="math/tex">y_i \in \mathcal{Y} \subseteq R</script>，<script type="math/tex">\mathcal{Y}</script>为输出空间。树可以表示为如下加法模型的形式:</p>
<script type="math/tex; mode=display">
T(x;\Theta) = \sum_{j=1}^J c_j I(x\in R_j) \tag{4-1}</script><p>其中，参数<script type="math/tex">\Theta=\{(R_1,c_1),(R_2,c_2),\cdots,(R_J,c_J)\}</script>表示树的区域划分和各区域上的常数。<strong>J是回归树的复杂度即叶节点个数。</strong></p>
<p>在前向分步算法的第m步，给定当前模型<script type="math/tex">f_{m-1}(x)</script>，需求解</p>
<script type="math/tex; mode=display">
\hat{\Theta}_m = \arg\min_{\Theta_m} \sum_{i=1}^N L(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))</script><p>得到<script type="math/tex">\hat{\Theta_m}</script>，即第m棵树的参数。</p>
<p>当采用<strong>平方误差损失函数</strong>时，</p>
<script type="math/tex; mode=display">
L(y,f(x)) = (y-f(x))^2</script><p>令<script type="math/tex">r = y - f_{m-1}(x)</script>，其损失变为：</p>
<script type="math/tex; mode=display">
\begin{aligned}L(y,f_{m-1}(x)+T(x;\Theta_m)) &= [y-f_{m-1}(x)-T(x;\Theta_m)]^2 \\ &= [r-T(x;\Theta_m)]^2\end{aligned}</script><p>这里，<script type="math/tex">r</script>是当前模型拟合数据的残差。<strong>所以，对回归问题的提升树来说，只需简单的拟合当前模型的残差</strong>。</p>
<p><strong>算法（回归问题的提升树算法)</strong></p>
<p>输入:训练数据集<script type="math/tex">T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\},x_i \in \mathcal{X} \subseteq R^n,y_i \in \mathcal{Y} \subseteq R</script>；</p>
<p>输出:提升树<script type="math/tex">f_M(x)</script></p>
<p><strong>(1)</strong>初始化<script type="math/tex">f_0(x)=0</script></p>
<p><strong>(2)</strong>对m=1,2,…,M</p>
<p>​    <strong>(a)</strong>计算残差<script type="math/tex">r = y - f_{m-1}(x)</script></p>
<script type="math/tex; mode=display">
r_{mi} = y_i - f_{m-1}(x_i) , i=1,2,\cdots,N</script><p>​    <strong>(b)</strong>拟合残差<script type="math/tex">r_{mi}</script>学习一个回归树，得到<script type="math/tex">T(x;\Theta_m)</script></p>
<p>​    <strong>(c)</strong>更新<script type="math/tex">f_m(x) = f_{m-1}(x)+T(x;\Theta_m)</script></p>
<p><strong>(3)</strong>得到回归问题提升树</p>
<script type="math/tex; mode=display">
f_M(x) = \sum_{m=1}^M T(x;\Theta_m)</script><h2 id="5-1-GBDT"><a href="#5-1-GBDT" class="headerlink" title="5.1 GBDT"></a>5.1 GBDT</h2><p>前面的回归问题提升树的前向分步算法的工作就是拟合当前模型的残差，在GBDT中，利用损失函数的负梯度在当前模型的值:</p>
<script type="math/tex; mode=display">
-\left[\frac{\partial{L(y_i,f(x_i))}}{\partial(f(x_i))} \right]_{f(x)=f_{m-1}(x)}</script><p>作为回归问题提升树算法中残差的近似值来拟合一个回归树。</p>
<p><strong>算法(梯度提升算法)</strong></p>
<p>输入:训练数据集<script type="math/tex">T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\},x_i \in \mathcal{X} \subseteq R^n,\mathcal{X}</script>为输入空间，<script type="math/tex">y_i \in \mathcal{Y} \subseteq R,\mathcal{Y}</script>为输出空间。损失函数<script type="math/tex">L(y,f(x))</script>；</p>
<p>输出:回归树<script type="math/tex">\hat{f}(x)</script>。</p>
<p><strong>(1)</strong>初始化</p>
<script type="math/tex; mode=display">
f_0(x) = \arg \min_c \sum_{i=1}^N L(y_i,c)</script><p><strong>(2)</strong>对m=1,2,…,M</p>
<p>​    <strong>(a)</strong>对i=1,2,…,N，计算</p>
<script type="math/tex; mode=display">
r_{mi} = -\left[\frac{\partial{L(y_i,f(x_i))}}{\partial(f(x_i))} \right]_{f(x)=f_{m-1}(x)}</script><p>​    <strong>(b)</strong>对<script type="math/tex">r_{mi}</script>拟合一个回归树，得到第m棵树的叶节点区域<script type="math/tex">R_{mj}</script>，j=1,2,…,J</p>
<p>​    <strong>(c)</strong>对j=1,2,…,J，计算</p>
<script type="math/tex; mode=display">
c_{mj} = \arg \min_c \sum_{x_i \in R_{mj}}L(y_i,f_{m-1}(x_i)+c)</script><p>​    <strong>(d)</strong>更新<script type="math/tex">f_m(x)=f_{m-1}(x)+\sum_{j=1}^J c_{mj}I(x\in R_{mj})</script></p>
<p><strong>(3)</strong>得到回归树</p>
<script type="math/tex; mode=display">
\hat{f}(x) = f_M(x) = \sum_{m=1}^M \sum_{j=1}^J c_{mj} I(x \in R_{mj})</script><p><strong>算法说明如下：</strong></p>
<p><strong>第1步</strong>初始化，估计使损失函数极小化的常数值，它是只有一个根节点的树。</p>
<p><strong>第2(a)步</strong>计算损失函数的负梯度在当前模型的值，将它作为残差的估计。(对于平方损失函数，它就是所谓的残差；对于一般损失函数，它是残差的近似值)</p>
<p><strong>第2(b)步</strong>估计回归树节点区域，以拟合残差的近似值。</p>
<p><strong>第2(c)步</strong>利用线性搜索估计叶节点区域的值，使损失函数极小化。</p>
<p><strong>第2(d)步</strong>更新回归树。</p>
<p><strong>第3步</strong>得到输出的最终模型<script type="math/tex">\hat{f}(x)</script>。</p>
<h2 id="6-1-XGBoost"><a href="#6-1-XGBoost" class="headerlink" title="6.1 XGBoost"></a>6.1 XGBoost</h2><h3 id="6-1-1-模型形式"><a href="#6-1-1-模型形式" class="headerlink" title="6.1.1 模型形式"></a>6.1.1 模型形式</h3><p>给定数据集<script type="math/tex">D={(X_i,y_i)}</script>(|D|=n,<script type="math/tex">x_i\in R^m,y_i \in R</script>)，xgboost利用前向分步算法，学习到包含K棵树的加法模型：</p>
<script type="math/tex; mode=display">
\hat{y}_i = \phi(X_i) = \sum_{k=1}^K f_k(X_i), f_k \in \mathcal{F} \tag{6-1}</script><p>此处<script type="math/tex">\mathcal{F} = \{f(X)=w_{q(X)}\}(q:R^m \rightarrow T,w\in R^T)</script>，此处的F表示基学习器为CART树的集合。T表示基学习器的叶子结点数。</p>
<p>每一个基学习器<script type="math/tex">f(X)</script>对应着一个树结构q(X)(它的意义是将样本X分到某个叶子结点上)；叶子结点的score向量<script type="math/tex">w=(w_0,w_1,\cdot,w_T)</script>，<script type="math/tex">w_i</script>表示第i个叶子结点对应的score。所以<script type="math/tex">w_{q(X)}</script>表示基学习器<script type="math/tex">f(X)</script>对样本X的预测值。</p>
<p><img src="image/xgboost-qx.png" alt="xgboost-qx"></p>
<p>那么，根据以上的加法模型，其最终的预测方式如下图所示，把每个基学习器预测到的结果(对应的叶子结点score值)累加起来作为最终的预测值:</p>
<p><img src="image/xgboost-predict.png" alt="xgboost-predict"></p>
<blockquote>
<p>回归树的预测输出是实数值，对于回归问题，可以直接作为预测值；对于分类问题，需要映射成概率，比如采用sigmoid函数，<script type="math/tex">\sigma(z) = \frac{1}{1+e^z}</script></p>
</blockquote>
<h3 id="6-1-2-目标函数"><a href="#6-1-2-目标函数" class="headerlink" title="6.1.2 目标函数"></a>6.1.2 目标函数</h3><script type="math/tex; mode=display">
\mathcal{L}(\phi) = \sum_i l(\hat{y}_i,y_i)+\sum_k \Omega(f_k) , \Omega(f) = \gamma T+\frac{1}{2} \lambda\|w\|^2 \tag{6-2}</script><p>目标函数中第一项为损失函数；第二项为正则项，对每棵回归树(基学习器)的复杂度进行了惩罚。</p>
<blockquote>
<p>损失函数可以是square loss,对数损失函数等，正则化项可以是L1,L2。</p>
</blockquote>
<p>根据前向分步算法，在前向分步算法 的第t步，给定前t-1棵回归树的情况下，第t棵回归树可表示为：</p>
<script type="math/tex; mode=display">
\hat{y}_i^{(t)} = \hat{y}_i{(t-1)}+f_t(x_i) \tag{6-3}</script><p>那么，第t步的目标函数可写成：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{L}^{(t)} &= \sum_i^n  l(y_i,\hat{y}_i^{(t)})+\sum_{k=1}^t \Omega(f_k)\\
&=\sum_i^n  l(y_i,\hat{y}_i^{(t-1)}+f_t(X_i))+\Omega(f_t)+constant
\end{aligned}
\tag{6-4}</script><p>如式(6-4)所示，第t步的目标函数变成了求解找到使得式(6-4)取最小值的<script type="math/tex">f_t</script>回归树。</p>
<blockquote>
<p>二阶泰勒展开：</p>
<script type="math/tex; mode=display">
f(x+\Delta x) \simeq f(x)+f^{'}(x)\Delta x+\frac{1}{2}f^{''}(x)\Delta x^2</script></blockquote>
<p>对式(6-4)进行二阶泰勒展开：</p>
<script type="math/tex; mode=display">
\mathcal{L}^{(t)} \simeq \sum_{i=1}^n [l(y_i,\hat{y}_i^{(t-1)})+g_if_t(X_i)+\frac{1}{2}h_if_t^2(X_i)]+\Omega(f_t)+constant \tag{6-5}</script><p>其中，<script type="math/tex">g_i = \partial_{\hat{y}^{(t-1)}}l(y_i,\hat{y}^{(t-1)})</script>，<script type="math/tex">h_i = \partial_{\hat{y}^{(t-1)}}^2 l(y_i,\hat{y}^{(t-1)})</script></p>
<p>去掉常数项，式(6-5)简化为:</p>
<script type="math/tex; mode=display">
\mathcal{L}^{(t)} \simeq \sum_{i=1}^n [g_if_t(X_i)+\frac{1}{2}h_if_t^2(X_i)]+\Omega(f_t) \tag{6-6}</script><p>在xgboost中，对基学习器<script type="math/tex">f_t</script>的复杂度定义为：</p>
<p><img src="image/xgboost-complexity.png" alt="xgboost-complexity"></p>
<p>定义叶子结点j所包含的样本集为<script type="math/tex">I_j=\{i|q(x_i)=j\}</script>，那么把树结构<script type="math/tex">f(X)</script>的定义式代入式(6-6)中，得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{L}^{(t)} &\simeq \sum_{i=1}^n [g_if_t(X_i)+\frac{1}{2}h_if_t^2(X_i)]+\Omega(f_t)\\
&= \sum_{i=1}^n [g_i w_{q(x_i)}+\frac{1}{2}h_i w_{q(x_i)}^2] +\gamma T+\lambda \frac{1}{2}\sum_{j=1}^T w_j^2 \\
&= \sum_{j=1}^T \left[(\sum_{i\in I_j}g_i)w_j+\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda)w_j^2\right]+\gamma T
\end{aligned} \tag{6-7}</script><p>根据一元二次方程的性质，令<script type="math/tex">G_j = \sum_{i\in I_j}g_i</script> ，<script type="math/tex">H_j = \sum_{i\in I_j}h_i</script>代入式(6-7)，得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{L}^{(t)} &= \sum_{j=1}^T \left[(\sum_{i\in I_j}g_i)w_j+\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda)w_j^2\right]+\gamma T \\
&= \sum_{j=1}^T \left[G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2\right]+\gamma T
\end{aligned} \tag{6-8}</script><p>给定树结构<script type="math/tex">q(x)</script>，每个叶子结点的score<script type="math/tex">w_j</script>的解为:</p>
<script type="math/tex; mode=display">
w_j^{*} = - \frac{G_j}{H_j+\lambda} \tag{6-9}</script><p>把式(6-9)代入式(6-8)，得：</p>
<script type="math/tex; mode=display">
L^{(t)} = -\frac{1}{2}\sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T \tag{6-10}</script><h3 id="6-1-3-学习策略"><a href="#6-1-3-学习策略" class="headerlink" title="6.1.3 学习策略"></a>6.1.3 学习策略</h3><p>当回归树的结果确定时，我们可以推导出最优叶结点分数式(6-9)以及对应的最小损失值式(6-10)，那么问题是怎么确定树的结构？</p>
<p>暴力枚举所有可能的树结构，选择损失值最小的，这是NP难问题。(不可行)</p>
<p>xgboost采用贪心算法，每次尝试分裂一个叶结点，计算分别前后的增益，选择增益最大的分裂点。那么如何定义增益函数？</p>
<p>首先，观察式(6-10)，<script type="math/tex">\frac{G_j^2}{H_j+\lambda}</script>衡量了叶子结点j对总体损失的贡献，我们希望损失越小越好，则标红部分的值越大越好(因为前面是负号)。</p>
<p>因此，对一个叶子结点进行分裂，分裂前后的增益函数定义为:</p>
<p><img src="image/xgboost-gain.png" alt="xgboost-gain"></p>
<p>增益函数也可表示成论文中的公式(7)的形式:</p>
<script type="math/tex; mode=display">
\mathcal{L}_{split} = \frac{1}{2}\left[\frac{(\sum_{i\in I_L}g_i)^2}{\sum_{i \in I_L}h_i+\lambda}+\frac{(\sum_{i\in I_R}g_i)^2}{\sum_{i \in I_R}h_i+\lambda}-\frac{(\sum_{i\in I}g_i)^2}{\sum_{i \in I}h_i+\lambda}\right] - \gamma</script><h3 id="6-1-4树结点分裂方法-split-finding"><a href="#6-1-4树结点分裂方法-split-finding" class="headerlink" title="6.1.4树结点分裂方法(split finding)"></a>6.1.4树结点分裂方法(split finding)</h3><h4 id="6-1-4-1-精确贪心算法"><a href="#6-1-4-1-精确贪心算法" class="headerlink" title="6.1.4.1 精确贪心算法"></a>6.1.4.1 精确贪心算法</h4><p>遍历所有的特征的所有可能的分裂点，计算gain值，选取值最大的(feature,value)去分割。分裂算法伪代码如下所示：</p>
<p><img src="image/xgboost-exact.png" alt="xgboost-exact"></p>
<p>时间复杂度为O(dKnlogn)，对每一个特征进行排序，排序时间为O(nlogn)，d维特征(相当于伪代码中的m)，树的深度为K。</p>
<p>假如现在枚举的是年龄特征<script type="math/tex">x_j</script>。现在考虑划分点a，因此要计算枚举<script type="math/tex">x_j<a</script>和<script type="math/tex">x_j \ge a</script> 的一阶梯度和：</p>
<p><img src="image/xgboost-split.png" alt="xgboost-split"></p>
<blockquote>
<p><strong>分裂不一定会使得情况变好</strong>，因为有一个引入新叶子的惩罚项<script type="math/tex">\gamma</script>，相当于进行树的剪枝操作。同时，还可以引入early stop策略，使得当引入新的分裂带来的增益小于一个阈值的时候，就不进行分裂操作了。</p>
</blockquote>
<h4 id="6-1-4-2-近似算法"><a href="#6-1-4-2-近似算法" class="headerlink" title="6.1.4.2 近似算法"></a>6.1.4.2 近似算法</h4><p>第一种精确算法的时间复杂度高，而且对于数据量十分庞大的时候，不能全部放入内存中，会使得精确贪心算法变得很慢。因此引入一种近似算法。</p>
<p>算法思想：对于每个特征，只考察候选切分点(分位点)，减少计算复杂度。根据候选切分点把当前结点的样本放入对应的桶中，对每个桶的G,H进行累加。最后在候选切分点集合上贪心查找，和精确贪心算法类似。算法描述如下：</p>
<p><img src="image/xgboost-approximate.png" alt="xgboost-approximate"></p>
<p>根据分位点给出对应的一组候选切分点后，算法的简单示例如下所示：</p>
<p><img src="image/xgboost-quantile-split.png" alt="xgboost-quantile-split"></p>
<p>那么，现在就有两个问题：</p>
<ol>
<li>什么时候进行候选切分点的选取？</li>
<li>如何选取候选切分点？</li>
</ol>
<p>上面的示例，给出了第二个问题的一个简单的解决方案，即采用分位数。</p>
<p>对于第一个问题，通过算法伪代码我们可以看到，有两种策略，全局策略(Global)和局部策略(Local):</p>
<ul>
<li>Global:学习每棵树前，提出候选切分点</li>
<li>Local:每次分裂前，重新提出候选切分点</li>
</ul>
<p>关于这两种策略以及精确贪心算法的对比情况，如下图：</p>
<p><img src="image/xgboost-approximate-split-2.png" alt="xgboost-approximate-split-2"></p>
<p>从上图可以看出(桶的个数等于1/eps):</p>
<ul>
<li>全局切分点的个数够多的时候，和精确贪心算法性能相当(global eps=0.05，几乎与exact greedy重合)</li>
<li>局部切分点个数不需要那么多，但是性能与exact greedy相差不多，计算复杂度介于exact greedy 和global 之间。</li>
</ul>
<p>对于第二个问题，xgboost并没有采用简单的分位数方法，而是提出一种以二阶梯度h为权重的分位数算法。比如，加权三分位切分点如下图所示：</p>
<p><img src="image/xgboost-split-1.png" alt="xgboost-split"></p>
<h3 id="6-1-5-切分点的选取-weighted-quantile-sketch"><a href="#6-1-5-切分点的选取-weighted-quantile-sketch" class="headerlink" title="6.1.5 切分点的选取-weighted quantile sketch"></a>6.1.5 切分点的选取-weighted quantile sketch</h3><p>首先，对特征k构造multi-set数据集：<script type="math/tex">D_k=(x_{1k},h_1),(x_{2k},h_2),\cdots,(x_{nk},h_n)</script>，其中<script type="math/tex">x_{ik}</script>表示样本i的特征k的取值，而<script type="math/tex">h_i</script>则表示对应的二阶梯度。</p>
<p>定义一个rank函数：</p>
<script type="math/tex; mode=display">
r_k(z) = \frac{1}{\sum_{(x,h)\in\mathcal{D}_k}h} \sum_{(x,h)\in \mathcal{D}_k,x<z} h \tag{6-11}</script><p>式(6-11)表示第k个特征 小于z的样本比例，相当于分位数的作用。而候选切分点<script type="math/tex">\{s_{k1},s_{k2},\cdots,s_{kl}\}</script>要求：</p>
<script type="math/tex; mode=display">
|r_k(s_{k,j})-r_k(s_{k,j+1})|<\varepsilon ,s_{k1}=\min_i x_{ik}, s_{kl} = \max_i x_{ik}</script><p>以上的公式相当于表示，让相邻两个候选点相差不超过给定阈值，因此总共可以把特征k分成<script type="math/tex">1/\varepsilon</script>份。</p>
<p>为什么使用二阶梯度加权？把式(6-6)写成以下形式：</p>
<p><img src="image/xgboost-split-quantile.png" alt="xgboost-split-quantile"></p>
<blockquote>
<p>推导第三行加入<script type="math/tex">\frac{g_t^2}{h_t^2}</script> 为常量，因此可以加入目标函数中，不影响目标函数的解。</p>
</blockquote>
<p>从上式中可以看出，hi有对loss加权的作用。</p>
<h3 id="6-1-6-稀疏值处理-sparsity-aware-split-finding"><a href="#6-1-6-稀疏值处理-sparsity-aware-split-finding" class="headerlink" title="6.1.6 稀疏值处理-sparsity-aware split finding"></a>6.1.6 稀疏值处理-sparsity-aware split finding</h3><p>稀疏数据的产生原因：</p>
<ol>
<li>数据缺失值</li>
<li>大量的0值</li>
<li>one-hot编码</li>
</ol>
<p>xgboost能对缺失值自动进行处理，其思想是对于缺失值自动学习出它该被划分的方向(左子树或右子树):</p>
<p><img src="image/xgboost-sparsity.png" alt="xgboost-sparsity"></p>
<p>上述的算法的简单说明如下：</p>
<ul>
<li>让特征k的所有缺失值的都到右子树，然后和之前的一样，枚举划分点，计算最大的gain</li>
<li>让特征k的所有缺失值的都到左子树，然后和之前的一样，枚举划分点，计算最大的gain</li>
</ul>
<p>这样最后求出最大增益的同时，也知道了缺失值的样本应该往左边还是往右边。使用了该方法，相当于比传统方法多遍历了一次，但是它只在非缺失值的样本上进行迭代，因此其复杂度与非缺失值的样本成线性关系。</p>
<h3 id="6-1-7-shrinkage和列抽样"><a href="#6-1-7-shrinkage和列抽样" class="headerlink" title="6.1.7 shrinkage和列抽样"></a>6.1.7 shrinkage和列抽样</h3><p>除了在目标函数中加入正则项来防止过拟合，还可以通过shinkage和列抽样技术来防止过拟合。</p>
<p>shrinkage技术即在每轮迭代生成的树结构中，对叶子结点乘以一个缩减权重<script type="math/tex">\eta</script>，该操作的作用就是减少每棵树的影响力，留更多的拟合空间给后来的树结构。</p>
<p>列抽样技术，它有两种实现方式，一种是按层随机，另一种是按树随机(构建树前就随机选择特征)。</p>
<p>对于按层随机方式，在每次分裂一个结点的时候，对同一层内的每个结点分裂之前，先随机选择一部分特征，这时候只需要遍历一部分特征，来确定最后分割点。</p>
<p>对于按树随机方式， 即构建树结构前就随机选择特征，之后所有叶子结点的分裂都只使用这部分特征。</p>
<blockquote>
<p>行抽样则是bagging的思想，每次只抽取部分的样本进行训练，而不是用全部样本，从而增加树的多样性。</p>
</blockquote>
<h3 id="6-1-8-系统设计"><a href="#6-1-8-系统设计" class="headerlink" title="6.1.8 系统设计"></a>6.1.8 系统设计</h3><h4 id="6-1-8-1-分块并行"><a href="#6-1-8-1-分块并行" class="headerlink" title="6.1.8.1 分块并行"></a>6.1.8.1 分块并行</h4><p>在建树的过程中，最耗时是找最优的切分点，而这个过程中，最耗时的部分是将数据排序。为了减少排序的时间，提出Block结构存储数据。</p>
<ul>
<li>Block中的数据以稀疏格式<strong>CSC</strong>进行存储</li>
<li>Block中的<strong>特征进行排序</strong>（不对缺失值排序）</li>
<li>Block 中特征还需存储指向样本的<strong>索引</strong>，这样才能根据特征的值来取梯度。</li>
<li>一个Block中存储一个或多个特征的值</li>
</ul>
<p><img src="image/xgboost-block.png" alt="xgboost-block"></p>
<p>可以看出，<strong>只需在建树前排序一次</strong>，后面节点分裂时可以直接根据索引得到梯度信息。</p>
<ul>
<li>在Exact greedy算法中，将整个数据集存放在一个Block中。这样，复杂度从原来的𝑂(𝐻𝑑||𝑥||0log𝑛)降为𝑂(𝐻𝑑||𝑥||0+||𝑥||0log𝑛)，其中||𝑥||0为训练集中非缺失值的个数。这样，Exact greedy算法就省去了每一步中的排序开销。</li>
<li>在近似算法中，使用多个Block，每个Block对应原来数据的子集。不同的Block可以在不同的机器上计算。该方法对Local策略尤其有效，因为Local策略每次分支都重新生成候选切分点。</li>
</ul>
<p>Block结构还有其它好处，数据按列存储，可以同时访问所有的列，很容易实现并行的寻找分裂点算法。此外也可以方便实现之后要讲的out-of score计算。</p>
<p>缺点是空间消耗大了一倍。</p>
<h4 id="6-1-8-2-缓存优化-cache-aware-access"><a href="#6-1-8-2-缓存优化-cache-aware-access" class="headerlink" title="6.1.8.2 缓存优化-cache-aware access"></a>6.1.8.2 缓存优化-cache-aware access</h4><p>使用Block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的。这将导致<strong>非连续</strong>的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。</p>
<p><img src="image/xgboost-cache-1.png" alt="xgboost-cache-1"></p>
<p>因此，对于exact greedy算法中, 使用<strong>缓存预取</strong>。具体来说，对每个线程分配一个连续的buffer，读取梯度信息并存入Buffer中（这样就实现了非连续到连续的转化），然后再统计梯度信息。该方式在训练样本数大的时候特别有用，见下图：</p>
<p><img src="image/xgboost-cache-2.png" alt="xgboost-cache-2"></p>
<p>在approximate 算法中，对Block的大小进行了合理的设置。定义Block的大小为Block中最多的样本数。设置合适的大小是很重要的，设置过大则容易导致命中率低，过小则容易导致并行化效率不高。经过实验，发现2^16比较好。</p>
<p><img src="image/xgboost-cache-3.png" alt="xgboost-cache-3"></p>
<h4 id="6-1-8-3-Blocks-for-Out-of-core-Computation"><a href="#6-1-8-3-Blocks-for-Out-of-core-Computation" class="headerlink" title="6.1.8.3 Blocks for Out-of-core Computation"></a>6.1.8.3 Blocks for Out-of-core Computation</h4><p>当数据量太大不能全部放入主内存的时候，为了使得<a href="https://en.wikipedia.org/wiki/External_memory_algorithm" target="_blank" rel="noopener">out-of-core</a>计算称为可能，将数据划分为多个Block并存放在磁盘上。</p>
<ul>
<li>计算的时候，使用独立的线程预先将Block放入主内存，因此可以在计算的同时读取磁盘</li>
<li>Block压缩，貌似采用的是近些年性能出色的LZ4 压缩算法，按列进行压缩，读取的时候用另外的线程解压。对于行索引，只保存第一个索引值，然后用16位的整数保存与该block第一个索引的差值。</li>
<li>Block Sharding， 将数据划分到不同硬盘上，提高磁盘吞吐率</li>
</ul>
<h2 id="7-1-参考资料"><a href="#7-1-参考资料" class="headerlink" title="7.1 参考资料"></a>7.1 参考资料</h2><ul>
<li><p><a href="http://wepon.me/files/gbdt.pdf" target="_blank" rel="noopener">GBDT算法原理与系统设计简介</a></p>
</li>
<li><p><a href="https://www.hrwhisper.me/machine-learning-xgboost/" target="_blank" rel="noopener">『我爱机器学习』集成学习（三）XGBoost</a></p>
</li>
<li><p><a href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf" target="_blank" rel="noopener">[Tianqi’s Slide]Introduction to Boosted Trees</a></p>
</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/XGBoost/" rel="tag"># XGBoost</a>
          
            <a href="/tags/Ensemble/" rel="tag"># Ensemble</a>
          
            <a href="/tags/Gradient-Boosting/" rel="tag"># Gradient Boosting</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/07/03/UCR-DTW和UCR-ED模型详解/" rel="next" title="UCR-DTW和UCR-ED模型详解">
                <i class="fa fa-chevron-left"></i> UCR-DTW和UCR-ED模型详解
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">jozee</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">25</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">30</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-决策树"><span class="nav-number">1.</span> <span class="nav-text">1.1 决策树</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-CART树"><span class="nav-number">2.</span> <span class="nav-text">2.1 CART树</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-AdaBoost与前向分步算法"><span class="nav-number">3.</span> <span class="nav-text">3.1 AdaBoost与前向分步算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-AdaBoost算法"><span class="nav-number">3.1.</span> <span class="nav-text">3.1.1 AdaBoost算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-前向分步算法"><span class="nav-number">3.2.</span> <span class="nav-text">3.1.2 前向分步算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-3-AdaBoost算法与前向分步算法的关系"><span class="nav-number">3.3.</span> <span class="nav-text">3.1.3 AdaBoost算法与前向分步算法的关系</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-回归问题提升树的前向分布算法"><span class="nav-number">4.</span> <span class="nav-text">4.1 回归问题提升树的前向分布算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-GBDT"><span class="nav-number">5.</span> <span class="nav-text">5.1 GBDT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1-XGBoost"><span class="nav-number">6.</span> <span class="nav-text">6.1 XGBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-1-模型形式"><span class="nav-number">6.1.</span> <span class="nav-text">6.1.1 模型形式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-2-目标函数"><span class="nav-number">6.2.</span> <span class="nav-text">6.1.2 目标函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-3-学习策略"><span class="nav-number">6.3.</span> <span class="nav-text">6.1.3 学习策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-4树结点分裂方法-split-finding"><span class="nav-number">6.4.</span> <span class="nav-text">6.1.4树结点分裂方法(split finding)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-4-1-精确贪心算法"><span class="nav-number">6.4.1.</span> <span class="nav-text">6.1.4.1 精确贪心算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-4-2-近似算法"><span class="nav-number">6.4.2.</span> <span class="nav-text">6.1.4.2 近似算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-5-切分点的选取-weighted-quantile-sketch"><span class="nav-number">6.5.</span> <span class="nav-text">6.1.5 切分点的选取-weighted quantile sketch</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-6-稀疏值处理-sparsity-aware-split-finding"><span class="nav-number">6.6.</span> <span class="nav-text">6.1.6 稀疏值处理-sparsity-aware split finding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-7-shrinkage和列抽样"><span class="nav-number">6.7.</span> <span class="nav-text">6.1.7 shrinkage和列抽样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-8-系统设计"><span class="nav-number">6.8.</span> <span class="nav-text">6.1.8 系统设计</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-8-1-分块并行"><span class="nav-number">6.8.1.</span> <span class="nav-text">6.1.8.1 分块并行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-8-2-缓存优化-cache-aware-access"><span class="nav-number">6.8.2.</span> <span class="nav-text">6.1.8.2 缓存优化-cache-aware access</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-8-3-Blocks-for-Out-of-core-Computation"><span class="nav-number">6.8.3.</span> <span class="nav-text">6.1.8.3 Blocks for Out-of-core Computation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-1-参考资料"><span class="nav-number">7.</span> <span class="nav-text">7.1 参考资料</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">jozee</span>

  
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
    </script>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_pv">
        本站总访问量: <span id="busuanzi_value_site_pv"></span>次
    </span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">
        本站访客数<span id="busuanzi_value_site_uv"></span>人次
    </span>
</div>









        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  

  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://jozeelin.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2019/07/19/XGBoost/';
          this.page.identifier = '2019/07/19/XGBoost/';
          this.page.title = 'XGBoost论文解读';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://jozeelin.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
